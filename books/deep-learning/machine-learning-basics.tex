
\chapter{Machine Learning Basics}
Machine learning is essentially a form of applied statistics with increased emphasis on the use of computers to statistically estimate complicated functions and a decreased emphasis on proving confidence intervals around these functions.


\section{Learning Algorithms}

A machine learning algorithm is an algorithm that is able to learn from data.
But what do we mean by learning?
Mitchell provides a succinct definition:
``A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$''
Now, how does the learn happen?
The model or algorithm learns by adjusting the parameters contained in it.


\section{Capacity, Overfitting and Underfitting}

The central challenge in machine learning is that out algorithm must perform well on \itwords{new, previously} -- not just on which our model was trained.
The ability to perform on previously unobserved inputs is called \keywords{generalization}.

Typically, when training a machine learning model, we have access to a training set;
we can compute some error measure on the training set, called the \keywords{training error};
and we reduce this training error.
We simply call this an optimization problem.
What separate machine learning from optimization is that we want the \keywords{generalization error}, also called the \keywords{test error} to be low as well.
The generalization error is defined as the expected value of the error on a new input.



How can we affect performance on the test set when we get to observe only the training set? 
The field of \keywords{statistical learning theory} provides some answers. 
If the training and the test set are collected arbitrarily, there is indeed little we can do.
If we are allowed to make some \keywords{assumptions} about how the training and test set are collected, then we can make some progress.

The training and test data are generated by a probability distribution over datasets called the \keywords{data-generating process}.
We typically make a set of assumptions known as the \keywords{i.i.d. assumptions}.
These assumptions are that the examples in each dataset are independent from each other, and that the training set and test set are identically distributed, drawn from the same probability distributed as each other.
This assumption enables us to descirbe the data-generating process with a probability distribution over a single example.
The same distribution is then used to generate every train example and every test example.
We call that shared underlying distribution the \keywords{data-generating distribution}, denoted $p_{data}$.
This probability framework and the i.i.d. assumptions enables us to mathematically study the relationship between training error and test error.


One connection between training error and test error is that expected training error of a randomly selected model is equal to the expected test error of that model.
When we use a machine learning algorithm, we sample the training set, the use it to choose the parameters to reduce training set error, the sample the test set.
Under this process, the expected test error is greater than or equal to the expected value of training error.
The factors determining how well a machine learning algorithm will perform are its ability to
\begin{enumerate}
\item Make the training error small.
\item Make the gap between training and test error small.
\end{enumerate}
These two factors correspond to the two central challenges: \keywords{underfitting} and \keywords{overfitting}.
Underfitting occurs when the model is not able to obtain a sufficient low error value on the training set.
Overfitting occurs when the gap between the training error and test error is too large.

We can control whether a model is more likely to overfit or underfit by altering its \keywords{capacity}.
Informally, a model's capacity is its ability to fit a wide variety of functions.


One way to control the capacity of a learning algorithm is by choosing its hypothesis space, the set of functions that the learning algorithm is allowed to selected as being the solution.
Capacity is not determined only by the choice of model.
The model specifies which family of functions the learning algorithm can choose from.
This is called the \keywords{representation capacity} of the model.
In many cases, finding the best function within this family is a difficult optimization problem.
In practice, the learning algorithm does not actually find the best function, but merely one that sinificantly reduces the training error.
This additional limitations, such as the inperfection of the optimization algorithm, mean that the learning algorithm's \keywords{effective capacity} may be less than the representational capacity of the model family.

\begin{tcolorbox}
  Machine learning algorithm will generally perform best when their capacity is appropriate for the true complexity of the task and the amount of training data.
\end{tcolorbox}



The ideal model is an oracle that simply knows the true probability distribution that generate the data.
Even such a model will still incur some error on many problems, because there may still be some noise in the distribution.
In the case of supervised learning, the mapping from $\bm{x}$ to $y$ may be inherently stocastic, or $y$ may be a deterministic function that involves other variables besides those included in $\bm{x}$.
The error incurred by an oracle making predictions from the true distribution $p(\bm{x},y)$ is called the \keywords{Bayes error}.


\subsection{The No Free Lunch Theorem}

Learning theory claims that a machine learning algorithm can generalize well from a finite training set of examples.
However, to logically infer a rule describing every memeber of a set, one must have information about every member of that set.

In part, machine learning avoids this problem by offering only probabilistic rules, rather than the entirely certain rules used in purely logical reasoning.
Machine learning promises to find rules that are \itwords{probably} correct about \itwords{most} memebers of the set they can concern.


Unfortunately, even this does not resovle the entire problem.
The \keywords{no free lunch theorem} for machine learning states that, averaged over all possible data-generating distribution, every classification algorithm has the same error when classicifying previously unobserved points.
In other words, in some sense, no machine learning algorithm is universally any better than any other.

Fortunately, these results holds only when we average over \itwords{all} possible data-generating distribution.
If we make assumptions about the kinds of probability distributions we encounter in real-world application, then we can design learning algorithms that perform well on these distributions.


This means that the goal of machine learning research is not to seek a universal learning algorithm or the absolute best learning algorithm.
Instead, our goal is to understand what kinds of distributions are relevant to the ``real world'' that an AI agent experiences, and what kinds of machine learning algorithm perform well on data drawn from the kinds of data-generating distributions we care about.


\subsection{Regularization}

The no free lunch theorem implies that we must design our machine learning algorithms to perform well on a specific task.
We do so by building a set of preferences into the learning algorithm.

The behavior of our algorithm is strongly affacted not just by how large we make the set of functions allowed in its hypothesis space, but by the specific identity of those functions.
For example, liner regression would not perform well if we tried to use it to predict $\sin(x)$ from $x$.
We can thus control the performance of our algorithm by choosing what kind of functions we allow them to draw solutions from, as well as by controlling the amount of these functions.

Generally, we can regularize a model that learns a function $f(\bm{x};\theta)$ by adding a penalty called a \keywords{regularizer} to the cost function.
In the case of weight decay, the relularizer is $\Omega(w) = w^\top w$.

Expressing preferences for one function over anohter is a more general way of controlling a model's capacity.

\begin{tcolorbox}
  Regularization is any modification we make to a learnining algorithm that is intended to reduce its generalization error but not its training error.
\end{tcolorbox}


The no free lunch theorem has made it clear that there is no best machine learning algorithm, and in particular, no best form of regularization.
Instead we must choose a form of regularization that is well suited to the particular task we want to solve.
The philosophy of deep learning is that a wide range of tasks may all be solved effectively using very general-purpose forms of regularization.

\section{Hyperparameters and validation sets}

Most machine learning algorithms have hyperparameters, setting that we can use to control the algorithm's behavior.
The values of hyperparameters are not adapted by the learning algorithm itself.


Sometimes a setting is chosen to be a hyperparameter that the learning algorithm does not learn because the setting is difficult to optimize.
Most frequently, the setting must be hyperparameter because it is not appropriate to learn that hyperparameter on the training set.
This applies ot all hyperparameters that control model capacity.
If learned on the trainning set, sunch hyperparameters would always choose the maximum possible capacity, resulting in overfitting.


To solve this problem, we need a \keywords{validation set} of examples that the training algorithm does not observe.
It is important that the test examples are not used in any way to make choice about the model, including hyperparameters.
Therefore, we always construct the validation set from the training data.
Specifically, we split the training data into tow disjoint subsets.
The subset of data used to learn the parameters is still typically called the training set.
The subset of data used to guide the selection of hyperparameters is called the validation set.
Typically, one uses about 80 percent of the training data for training and 20 percent for validation.

\section{Estimators, bias and variance}

Foundational concepts such as parameter estimation, bias and variance are useful to formally characterize notions of generalization, underfitting and overfitting.

\subsection{Point estimation}
Point estimation is the attempt to provide the single ``best'' prediction of some quantity of interest.
To distinguish estimators of parameters from their true value, our convention will be to denote a point estimate of a parameter $\bm{\theta}$ by $\hat{\bm{\theta}}$.

Let $\{\bm{x}^{(1)},\dots,\bm{x}^{(m)}\}$ be a set of $m$ independent and identically distributed (i.i.d.) data points.
A \keywords{point estimator} or \keywords{statistic} is any function of the data:
\begin{equation}
  \hat{\bm{\theta}} = g(\bm{x}^{(1)},\dots,\bm{x}^{(m)}).
\end{equation}
While almost any function thus qualifies as an estimator, a good estimator is a function whose output is close to the true underlying $\bm{\theta}$ that generated the training data.

\subsection{Bias}

The bias of an estimator is defined as
\begin{equation}
  \mathrm{bias}(\hat{\bm{\theta}}_m) = \mathbb{E}(\hat{\bm{\theta}}_m) - \bm{\theta},
\end{equation}
where the expectation is over the data (seen as samples from a random variable) and $\bm{\theta}$ is the true underlying value of $\bm{\theta}$ used to define the data-generating distribution.
An estimator $\hat{\bm{\theta}}$ is said to be \keywords{unbiased} if $\mathrm{bias}(\hat{\bm{\theta}}_m)=0$.
An estimator  $\hat{\bm{\theta}}_m$ is said to be \keywords{asymptotically unbiased} if $\lim_{m\rightarrow \infty}\mathrm{bias}(\hat{\bm{\theta}}_m) = 0$.

\subsection{Variance and standard error}

The variance of an estimator is simply the variance
\begin{equation}
  \mathrm{Var}(\hat{\bm{\theta}})
\end{equation}
where the random variable is the training set.
Alternately, the square root of the variance is called the \keywords{standard error}, denoted $\mathrm{SE}(\hat{\bm{\theta}})$.

The variance, or the standard error, of an estimator provides a measure of how we would expect the estimate we compute from data to vary as we independently resample the dataset from the underlying data-generating process.


\begin{tcolorbox}
  When we compute any statistic using a finite number of samples, our estimate of the true underlying parameter is uncertain.
\end{tcolorbox}


\subsection{Consistency}

We usually wish that, as the number of data points $m$ in our dataset increase, our point estimates converge to the true value of the corresponding parameters.
More formally, we would like that
\begin{equation}
  \label{eq:consistency}
  \mathrm{plim}_{m\rightarrow \infty}\hat{\bm{\theta}}_m = \bm{\theta}.
\end{equation}

The symbol plim indicates convergence in probability, meaning that for any $\epsilon > 0$, $P(|\hat{\bm{\theta}}_m - \bm{\theta}| > \epsilon) \rightarrow 0$ as $m \rightarrow \infty$.
The condition described by equation \ref{eq:consistency} is known as \keywords{consistency}.


\section{Maximum likelihood estimation (*)} 

Rather than guessing that some function might make a good estimator and then analyzing its bias and variance, we would like to have some priciple from which we can derive specific functions that are good estimators for different models.

The most common such principle is the maximum likelihood estimation.


Consider a set of $m$ examples $\mathbb{X} = \{\bm{x}^{(1)},\dots, \bm{x}^{(m)}\}$ drawn independently from the true but unknown data-generating distribution $p_{data}(\mathbf{x})$.

Let $p_{model}(\mathbf{x};\theta)$ be a parametric family of probability distributions over the same space indexed by $\theta$.

The maximum likelihood estimator for $\theta$ is then defined as:
\begin{equation}
  \label{eq:maximum-likelihood}
  \bm{\theta}_{\mathrm{ML}} = \mathop{\arg\max}_{\bm{\theta}} p_{\mathrm{model}}(\mathbb{X};\bm{\theta}) =  {\arg\max}_{\bm{\theta}} \prod_{i=1}^m p_{\mathrm{model}}(x^{(i)};\bm{\theta}) 
\end{equation}

To get a convenient but equivalent optimization problem:
\begin{equation}
  \bm{\theta}_{\mathrm{ML}} = \mathop{\arg\max}_{\bm{\theta}}  \sum_{i=1}^m  \log p_{\mathrm{model}}(x^{(i)};\bm{\theta})
\end{equation}

Because the arg max does not change when we rescale the cost function, we can divide by $m$ to obtain a version of the criterion that is expressed as expectation with respect to the empirical distribution $\hat{p}_{data}$ defined by the training data:
\begin{equation}
  \bm{\theta}_{\mathrm{ML}} = \mathop{\arg\max}_{\bm{\theta}}  \mathbb{E}_{x\sim \hat{p}_{data}}  \log p_{\mathrm{model}}(x;\bm{\theta})
\end{equation}


One way to interpret maximum likelihood estimation is to view it as minimizing the dissimilarity between the empirical distribution $\hat{p}_{data}$, defined by the training set and the model distribution, with the degree of dissimilarity between the two measured by the KL divergence.
The KL divergence is given by:
\begin{equation}
  \label{eq:kl-divergence}
  D_{KL}(\hat{p}_{data}||p_{data}) = \mathbb{E}_{\mathrm{x}\sim \hat{p}_{data}} [\log \hat{p}_{data}(\bm{x}) - \log p_{model}(\bm{x})]
\end{equation}

\subsection{Properties of maximum likelihood}

The main appeal of the maximum likelihood estimator is that it can be shown to be the best estimator asymptotically, as the number of examples $m \rightarrow \infty$, in terms of its rate of convergence as $m$ increases.

Under appropriate conditions, the maximum likelihood estimator has the property of consistency.
These conditions are as follows:
\begin{itemize}
\item The true distribution $p_{data}$ must lie within the model family $p_{model}(\cdot;\theta)$.
\item The true distribution $p_{data}$ must correspond to exactly one value of $\theta$.
\end{itemize}



\section{Bayesian statistics}

In frequentist perspective, the true parameter value $\theta$ is fixed but unknown, while the point estimator $\hat{\theta}$ is a random variable on account of it being a function of the dataset.
The Bayesian perspective on statistic is quite different.
The Bayesian uses probability to reflect degrees of certainty in states of knowledge.
The dataset is directly observed and so is not random.
On the other hand, the true parameter $\theta$ is unknown or uncertain and thus is represented as a random variable.



Before observing the data, we represent out \keyword{knowledge} of $\theta$ using the \keyword{prior probability distribution}, $p(\theta)$.
Generally, the machine learning practitioner selects a prior distribution that is quite broad (i.e. with high entropy) to reflect a high degree of uncertainty in the value of $\theta$ before observing any data.
Now consider that we have a set of data samples $\{x^{(1)}, \cdots,x^{(m)}\}$.
We can recover the effect of data on our belief about $\theta$ by combining the data likelihood $p(x^{(1)},\cdots,x^{(m)}|\theta)$ with the prior via Bayes' rule:
\begin{equation}
  p(\theta|x^{(1)},\cdots,x^{(m)}) = \frac{p(x^{(1)},\cdots,x^{(m)}|\theta)p(\theta)}{p(x^{(1)},\cdots,x^{(m)})}
\end{equation}







\section{Stochastic Gradient Descent}

Nearly all of deep learning is powered by one very important algorithm: \keyword{stochastic gradient descent} (SGD).
A recurring problem in machine learning is that large training sets are neccessary for good generalization, but large training sets are also more computationally expensive.


The cost function used by a machining learning algorithm often decomposes as a sum over training examples of some per-example loss function.
For example, the negative conditional log-likelihood of the training data can be written as
\begin{equation}
  J(\bm{\theta}) = \mathbb{E}_{\mathrm{x},y \sim \hat{p}_{data}} L(\bm{x},y,\bm{\theta}) = \frac{1}{m}\sum_{i=1}^m L(\bm{x}^{(i)},y^{(i)},\bm{\theta}),
\end{equation}
where L is the per-example loss $L(\bm{x},y,\bm{\theta}) = -\log p(y|\bm{x};\bm{\theta})$.

For these additive cost function, gradient requires computing
\begin{equation}
  \nabla_{\bm{\theta}} J(\bm{\theta}) = \frac{1}{m}\sum_{i=1}^m L(\bm{x}^{(i)},y^{(i)},\bm{\theta}).
\end{equation}
The computational cost of this operation is $O(m)$.
As the training set size grows to billions of examples, the time to take a single gradient step becomes prohibitively long.

The insight of SGD is that the gradient is an expectation.
The expectation may be approximately estimated using a small set of samples.
Specifically, on each step of the algorithm, we can sample a \keyword{minibatch} of examples $\mathbb{B}=\{\bm{x}^{(1)},\cdots,\bm{x}^{(m)}\}$ drawn uniformly from the training set.
The minibatch size $m'$ is typically chosen to be a relatively small number of examples, ranging from one to a few hundred.

The estimator of the gradient is formed as
\begin{equation}
  \bm{g} = \frac{1}{m'} \nabla_{\bm{\theta}} \sum_{i=1}^{m'} L(\bm{x}^{(i)},y^{(i)},\bm{\theta})
\end{equation}
using examples from the minibatch $\mathbb{B}$.
The stocastic gradient descent algorithm then follows the estimated gradient donwhill:
\begin{equation}
  \bm{\theta} \leftarrow \bm{\theta} - \epsilon \bm{g},
\end{equation}
where $\epsilon$ is the learning rate.

\section{Building a machine learning algorithm}

Nearly all deep learning algorithms can be described as particular instances of a fairly simple recipe:
\begin{itemize}
\item a specification of a dataset
\item a cost function
\item an optimization procedure
\item a model
\end{itemize}



\section{Challenges Motivating Deep Learning}

The development of deep learning was motivated in part by the failure of traditional algorithms to generalize well on AI tasks, such as recognizing speech or recognizing objects.

\subsection{The Curse of dimensionality}

Many machine learning problems become exceedingly difficult when the number of dimenions in the data is high.
This phonomenon is known as the \keyword{curse of dimensionality}.
One challenge posed by the curse of dimensionality is a statistical challenge.
A statistical challenge arises becuase the number of possible configurations of $x$ is much large than the number of training examples.


\subsection{Local constancy and smoothness regularization}

To generalize well, machine learning algorithms need to be guided by prior beliefs about what kind of function they should learn.
Among the most widely used implicit ``priors'' is the \keyword{smoothness prior}, or local constancy prior.
This prior states that the function we learn should not change very much within a small region.

There are many different ways to implicitly or explicitly express a prior belief that the learned function should be smooth or locally constant.
All these different methods are designed to encourage the learning process to learn a function $f^{*}$ that satisfies the condition
\begin{equation}
  f^{*}(\bm{x}) \approx f^{*}(\bm{x} + \epsilon)
\end{equation}

\section{Manifold Learning}

An important concept underlying many ideas in machine learning is that of a manifold.
A \keyword{manifold} is a connected region.
Mathematically, it is a set of points associated with a neighborhood around each point.
The concept of a neighborhood surrounding each point implies the existence of transformations that can be applied to move on the manifold from one position to a neighboring one.


Many machine learning problems seem hopeless if we expect the machine learning algorithm to learn function with interesting variations across all of $\mathbb{R}^n$.
\keyword{Manifold learning} algorithms surmount this obstacle by assuming
the most of $\mathbb{R}^n$ consists of invalid inputs,and that
interesting inputs occurs only along a collection of manifolds contaning a small subset of points,
with interesting variations in the output of the learned function occuring only along directions that lie on the manifold,
or with interesting variations happening only when we move from one manifold to another.


The assumption that the data lies along a low-dimensional manifold may not always be correct or useful.
We argue that in the context of AI tasks, such as those that involve processing images, sounds, or text,
the manifold assumptions is at least approximately correct.

The evidence in favor of this assumptions consists of tow categories of observations:
\begin{enumerate}
\item The probability distribution over images, text strings, and sounds that occur in real life is highly concentrated.
\item We can imagine such neighborhoods and transformations, at least informally.
\end{enumerate}


