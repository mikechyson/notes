<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>ai-introduction</title>
<!-- 2019-12-09 Mon 15:17 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">ai-introduction</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. AI的概述</a>
<ul>
<li><a href="#sec-1-1">1.1. AI, ML, DL的关系</a></li>
<li><a href="#sec-1-2">1.2. AI的特点</a></li>
<li><a href="#sec-1-3">1.3. AI模型的解释</a></li>
<li><a href="#sec-1-4">1.4. 如果提高AI预测精度</a></li>
<li><a href="#sec-1-5">1.5. AI与人类学习</a></li>
<li><a href="#sec-1-6">1.6. 现状</a></li>
</ul>
</li>
<li><a href="#sec-2">2. 机器学习的的流程</a>
<ul>
<li><a href="#sec-2-1">2.1. 数据的加载</a>
<ul>
<li><a href="#sec-2-1-1">2.1.1. numpy</a></li>
<li><a href="#sec-2-1-2">2.1.2. pandas</a></li>
<li><a href="#sec-2-1-3">2.1.3. pyspark</a></li>
</ul>
</li>
<li><a href="#sec-2-2">2.2. 特征工程</a>
<ul>
<li><a href="#sec-2-2-1">2.2.1. 问题：无用特征</a></li>
<li><a href="#sec-2-2-2">2.2.2. 问题：数据无法直接进行模型训练</a></li>
<li><a href="#sec-2-2-3">2.2.3. 问题：模型调参无法提高</a></li>
<li><a href="#sec-2-2-4">2.2.4. 问题：样本不平衡</a></li>
<li><a href="#sec-2-2-5">2.2.5. 问题：过拟合</a></li>
<li><a href="#sec-2-2-6">2.2.6. 问题：欠拟合</a></li>
</ul>
</li>
<li><a href="#sec-2-3">2.3. 模型训练</a>
<ul>
<li><a href="#sec-2-3-1">2.3.1. 数据观察</a></li>
<li><a href="#sec-2-3-2">2.3.2. 模型选择</a></li>
<li><a href="#sec-2-3-3">2.3.3. 参数优化</a></li>
</ul>
</li>
<li><a href="#sec-2-4">2.4. 使用模型</a>
<ul>
<li><a href="#sec-2-4-1">2.4.1. sklearn</a></li>
<li><a href="#sec-2-4-2">2.4.2. tensorflow</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sec-3">3. 统计学习</a>
<ul>
<li><a href="#sec-3-1">3.1. 基于概率的信任</a></li>
<li><a href="#sec-3-2">3.2. 为什么用统计学？</a></li>
<li><a href="#sec-3-3">3.3. 什么是学习？</a></li>
<li><a href="#sec-3-4">3.4. 什么是统计学习？</a></li>
<li><a href="#sec-3-5">3.5. 统计学习三要素</a>
<ul>
<li><a href="#sec-3-5-1">3.5.1. 假设空间</a></li>
<li><a href="#sec-3-5-2">3.5.2. 优化目标</a></li>
<li><a href="#sec-3-5-3">3.5.3. 求解算法</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sec-4">4. Algorithms</a>
<ul>
<li><a href="#sec-4-1">4.1. LMS (least mean square)</a></li>
<li><a href="#sec-4-2">4.2. KNN</a>
<ul>
<li><a href="#sec-4-2-1">4.2.1. 三个要素：</a></li>
<li><a href="#sec-4-2-2">4.2.2. 实现：</a></li>
<li><a href="#sec-4-2-3">4.2.3. 优缺点</a></li>
</ul>
</li>
<li><a href="#sec-4-3">4.3. Naive Bayes （朴素贝叶斯）</a>
<ul>
<li><a href="#sec-4-3-1">4.3.1. 理论推导</a></li>
<li><a href="#sec-4-3-2">4.3.2. Gaussian Naive Bayes</a></li>
<li><a href="#sec-4-3-3">4.3.3. 优缺点</a></li>
<li><a href="#sec-4-3-4">4.3.4. 常用领域</a></li>
</ul>
</li>
<li><a href="#sec-4-4">4.4. SVM (support vector machine)</a>
<ul>
<li><a href="#sec-4-4-1">4.4.1. 线性可分支持向量机</a></li>
</ul>
</li>
<li><a href="#sec-4-5">4.5. Decision Tree</a>
<ul>
<li><a href="#sec-4-5-1">4.5.1. id3</a></li>
<li><a href="#sec-4-5-2">4.5.2. c4.5</a></li>
<li><a href="#sec-4-5-3">4.5.3. cart</a></li>
<li><a href="#sec-4-5-4">4.5.4. 剪枝</a></li>
<li><a href="#sec-4-5-5">4.5.5. 损失函数</a></li>
<li><a href="#sec-4-5-6">4.5.6. 正则化</a></li>
<li><a href="#sec-4-5-7">4.5.7. Engineering</a></li>
<li><a href="#sec-4-5-8">4.5.8. 优缺点</a></li>
</ul>
</li>
<li><a href="#sec-4-6">4.6. Random Forest</a>
<ul>
<li><a href="#sec-4-6-1">4.6.1. 特点</a></li>
<li><a href="#sec-4-6-2">4.6.2. 构建</a></li>
<li><a href="#sec-4-6-3">4.6.3. Engineering</a></li>
</ul>
</li>
<li><a href="#sec-4-7">4.7. Boost</a>
<ul>
<li><a href="#sec-4-7-1">4.7.1. <span class="done DONE">DONE</span> adaboost</a></li>
<li><a href="#sec-4-7-2">4.7.2. <span class="todo TODO">TODO</span> gradient boost</a></li>
</ul>
</li>
<li><a href="#sec-4-8">4.8. Linear Regression</a>
<ul>
<li><a href="#sec-4-8-1">4.8.1. 目标函数</a></li>
<li><a href="#sec-4-8-2">4.8.2. 损失函数</a></li>
<li><a href="#sec-4-8-3">4.8.3. 梯度下降法求解损失函数</a></li>
<li><a href="#sec-4-8-4">4.8.4. batch or not</a></li>
<li><a href="#sec-4-8-5">4.8.5. simple code</a></li>
</ul>
</li>
<li><a href="#sec-4-9">4.9. 逻辑回归  (Logistic Regression)</a>
<ul>
<li><a href="#sec-4-9-1">4.9.1. 目标函数</a></li>
<li><a href="#sec-4-9-2">4.9.2. 损失函数</a></li>
<li><a href="#sec-4-9-3">4.9.3. 梯度下降求最优参数</a></li>
<li><a href="#sec-4-9-4">4.9.4. 逻辑回归多分类的实现：</a></li>
<li><a href="#sec-4-9-5">4.9.5. SVM vs LR</a></li>
</ul>
</li>
<li><a href="#sec-4-10">4.10. Softmax Regression</a>
<ul>
<li><a href="#sec-4-10-1">4.10.1. 目标函数</a></li>
<li><a href="#sec-4-10-2">4.10.2. <span class="done DONE">DONE</span> 损失函数</a></li>
<li><a href="#sec-4-10-3">4.10.3. <span class="todo TODO">TODO</span> 梯度下降法求解</a></li>
</ul>
</li>
<li><a href="#sec-4-11">4.11. EM</a>
<ul>
<li><a href="#sec-4-11-1">4.11.1. 聚类</a></li>
<li><a href="#sec-4-11-2">4.11.2. 实验</a></li>
<li><a href="#sec-4-11-3">4.11.3. 分析</a></li>
<li><a href="#sec-4-11-4">4.11.4. E - 隐藏变量的分布</a></li>
<li><a href="#sec-4-11-5">4.11.5. M - 参数最大化</a></li>
<li><a href="#sec-4-11-6">4.11.6. 例子</a></li>
<li><a href="#sec-4-11-7">4.11.7. <span class="done DONE">DONE</span> code</a></li>
<li><a href="#sec-4-11-8">4.11.8. 多解性(how to understand this?)</a></li>
</ul>
</li>
<li><a href="#sec-4-12">4.12. LDA</a></li>
<li><a href="#sec-4-13">4.13. 协同过滤推荐(collaborative filtering)</a>
<ul>
<li><a href="#sec-4-13-1">4.13.1. used-based CF</a></li>
<li><a href="#sec-4-13-2">4.13.2. item-based CF</a></li>
<li><a href="#sec-4-13-3">4.13.3. model-based CF</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sec-5">5. 距离</a>
<ul>
<li><a href="#sec-5-1">5.1. Euclidean Distance</a></li>
<li><a href="#sec-5-2">5.2. Cosine Distance</a></li>
<li><a href="#sec-5-3">5.3. Manhattan Distance</a></li>
<li><a href="#sec-5-4">5.4. Mahalanob Distance</a></li>
<li><a href="#sec-5-5">5.5. PearsonCorrelation</a></li>
<li><a href="#sec-5-6">5.6. KL散度 - 相对熵</a></li>
<li><a href="#sec-5-7">5.7. 交叉熵</a></li>
<li><a href="#sec-5-8">5.8. Hamming Distance</a></li>
<li><a href="#sec-5-9">5.9. Edit Distance</a></li>
<li><a href="#sec-5-10">5.10. Chebyshev Distance</a></li>
<li><a href="#sec-5-11">5.11. Inner Distance</a></li>
<li><a href="#sec-5-12">5.12. Jaccard Distance</a></li>
</ul>
</li>
<li><a href="#sec-6">6. 正则化</a>
<ul>
<li><a href="#sec-6-1">6.1. L-P范数</a></li>
<li><a href="#sec-6-2">6.2. L0</a></li>
<li><a href="#sec-6-3">6.3. L1</a></li>
<li><a href="#sec-6-4">6.4. L2</a></li>
<li><a href="#sec-6-5">6.5. 正则化求解</a></li>
<li><a href="#sec-6-6">6.6. Lasso</a></li>
<li><a href="#sec-6-7">6.7. Ridge</a></li>
</ul>
</li>
<li><a href="#sec-7">7. 牛顿法</a></li>
<li><a href="#sec-8">8. 频率学派和Bayes学派</a>
<ul>
<li><a href="#sec-8-1">8.1. 最大似然估计</a></li>
<li><a href="#sec-8-2">8.2. 最大后验概率</a></li>
<li><a href="#sec-8-3">8.3. 规律与实验次数</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> AI的概述</h2>
<div class="outline-text-2" id="text-1">
</div><div id="outline-container-sec-1-1" class="outline-3">
<h3 id="sec-1-1"><span class="section-number-3">1.1</span> AI, ML, DL的关系</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Artificial Intelligence &gt; Machine Learning &gt; Deep Learning<br  />
人工智能的范围很宽，日常中的AI常常指的是机器学习（ML）和深度学习（DL）。<br  />
</p>
</div>
</div>
<div id="outline-container-sec-1-2" class="outline-3">
<h3 id="sec-1-2"><span class="section-number-3">1.2</span> AI的特点</h3>
<div class="outline-text-3" id="text-1-2">
<ol class="org-ol">
<li>可解释性差<br  />
</li>
<li>准确性很难100%<br  />
</li>
</ol>
</div>
</div>
<div id="outline-container-sec-1-3" class="outline-3">
<h3 id="sec-1-3"><span class="section-number-3">1.3</span> AI模型的解释</h3>
<div class="outline-text-3" id="text-1-3">
<ul class="org-ul">
<li>几何角度<br  />
</li>
<li>概率角度<br  />
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-1-4" class="outline-3">
<h3 id="sec-1-4"><span class="section-number-3">1.4</span> 如果提高AI预测精度</h3>
<div class="outline-text-3" id="text-1-4">
<ol class="org-ol">
<li>优化模型参数<br  />
</li>
<li>更改模型<br  />
</li>
<li>做特征工程<br  />
</li>
<li>增加数据样本量<br  />
</li>
</ol>
</div>
</div>
<div id="outline-container-sec-1-5" class="outline-3">
<h3 id="sec-1-5"><span class="section-number-3">1.5</span> AI与人类学习</h3>
<div class="outline-text-3" id="text-1-5">
<p>
统计学是正确的学习方式。<br  />
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="left" />

<col  class="left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="left">&#xa0;</th>
<th scope="col" class="left">人类</th>
<th scope="col" class="left">机器</th>
</tr>
</thead>
<tbody>
<tr>
<td class="left">学习方式</td>
<td class="left">归纳 -&gt; 演绎</td>
<td class="left">从数据样本中归纳出模型，利用模型来进行预测</td>
</tr>

<tr>
<td class="left">目的</td>
<td class="left">抓住规律本质而不是表象</td>
<td class="left">不仅仅是拟合数据，而是要泛化。</td>
</tr>

<tr>
<td class="left">擅长</td>
<td class="left">逻辑运算</td>
<td class="left">数值运算</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-sec-1-6" class="outline-3">
<h3 id="sec-1-6"><span class="section-number-3">1.6</span> 现状</h3>
<div class="outline-text-3" id="text-1-6">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="left" />
</colgroup>
<tbody>
<tr>
<td class="left">应用层</td>
<td class="left">互联网，安防，医疗，金融，运营商</td>
</tr>

<tr>
<td class="left">接口层</td>
<td class="left">Tensorflow, Scikit-Learn, Spark MLlib</td>
</tr>

<tr>
<td class="left">算法层</td>
<td class="left">卷积神经网络，循环神经网络，随机森林，svm，线性回归等</td>
</tr>

<tr>
<td class="left">框架层</td>
<td class="left">Tensorflow</td>
</tr>

<tr>
<td class="left">基础层</td>
<td class="left">CPU与GPU集群，云平台</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>



<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> 机器学习的的流程</h2>
<div class="outline-text-2" id="text-2">
<p>
<a href="https://github.com/mikechyson/ai/tree/master/iris">https://github.com/mikechyson/ai/tree/master/iris</a><br  />
</p>
</div>
<div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1"><span class="section-number-3">2.1</span> 数据的加载</h3>
<div class="outline-text-3" id="text-2-1">
</div><div id="outline-container-sec-2-1-1" class="outline-4">
<h4 id="sec-2-1-1"><span class="section-number-4">2.1.1</span> numpy</h4>
<div class="outline-text-4" id="text-2-1-1">
<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">import</span> numpy <span style="color: #fff59d;">as</span> np

<span style="color: #ffcc80;">data</span> = np.load(<span style="color: #9ccc65;">'data.npz'</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-sec-2-1-2" class="outline-4">
<h4 id="sec-2-1-2"><span class="section-number-4">2.1.2</span> pandas</h4>
<div class="outline-text-4" id="text-2-1-2">
<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">import</span> pandas <span style="color: #fff59d;">as</span> pd

<span style="color: #ffcc80;">data</span> = pd.read_csv(<span style="color: #9ccc65;">'data.csv'</span>)
<span style="color: #ffcc80;">data</span> = pd.read_json(<span style="color: #9ccc65;">'data.csv'</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-sec-2-1-3" class="outline-4">
<h4 id="sec-2-1-3"><span class="section-number-4">2.1.3</span> pyspark</h4>
<div class="outline-text-4" id="text-2-1-3">
<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">from</span> pyspark <span style="color: #fff59d;">import</span> SparkConf, SparkContext

<span style="color: #ffcc80;">conf</span> = SparkConf().setAppName(<span style="color: #9ccc65;">'AppName'</span>)
<span style="color: #ffcc80;">sc</span> = SparkContext(conf=conf)

<span style="color: #ffcc80;">data</span> = sc.textFile(<span style="color: #9ccc65;">'data.txt'</span>)

<span style="color: #fff59d;">from</span> pyspark.sql <span style="color: #fff59d;">import</span> SparkSession

<span style="color: #ffcc80;">spark</span> = SparkSession.builder.appName(<span style="color: #9ccc65;">'appName'</span>).getOrCreate()
<span style="color: #ffcc80;">data</span> = spark.read.<span style="color: #ff8A65;">format</span>(<span style="color: #9ccc65;">'libsvm'</span>).load(<span style="color: #9ccc65;">'data.txt'</span>)
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2"><span class="section-number-3">2.2</span> 特征工程</h3>
<div class="outline-text-3" id="text-2-2">
<p>
重要：<br  />
不是为了特征工程而进行特征工程。<br  />
特征工程为了解决问题，所以当问题出现时才进行特征工程。<br  />
特征工程是以结果为导向的，<br  />
<a href="https://github.com/hackchyson/ai/tree/master/feature">https://github.com/hackchyson/ai/tree/master/feature</a><br  />
</p>
</div>

<div id="outline-container-sec-2-2-1" class="outline-4">
<h4 id="sec-2-2-1"><span class="section-number-4">2.2.1</span> 问题：无用特征</h4>
<div class="outline-text-4" id="text-2-2-1">
<p>
目的：简化模型，提高精度<br  />
数据中类似id等的特征，未进行模型训练之前就可确定，该属性对模型是没有影响的。<br  />
</p>
</div>
</div>

<div id="outline-container-sec-2-2-2" class="outline-4">
<h4 id="sec-2-2-2"><span class="section-number-4">2.2.2</span> 问题：数据无法直接进行模型训练</h4>
<div class="outline-text-4" id="text-2-2-2">
<p>
目的：让算法能正常运行。<br  />
</p>
</div>
<ol class="org-ol"><li><a id="sec-2-2-2-1" name="sec-2-2-2-1"></a>字符串<br  /><div class="outline-text-5" id="text-2-2-2-1">
<p>
模型对字符串的支持不好，需要转成向量。<br  />
</p>
</div>
<ol class="org-ol"><li><a id="sec-2-2-2-1-1" name="sec-2-2-2-1-1"></a>onehot<br  /><div class="outline-text-6" id="text-2-2-2-1-1">
<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">from</span> sklearn.preprocessing <span style="color: #fff59d;">import</span> Normalizer

<span style="color: #ffcc80;">data</span> = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
<span style="color: #fff59d;">print</span>(<span style="color: #9ccc65;">'data: '</span>, data)
<span style="color: #ffcc80;">normalizer</span> = Normalizer()
normalizer.fit(data)
<span style="color: #fff59d;">print</span>(normalizer.transform(data))
<span style="color: #fff59d;">print</span>(normalizer.transform([[2, 2]]))
</pre>
</div>

<p>
output:<br  />
</p>
<pre class="example">
[[5 8 9]
 [5 0 0]
 [1 7 6]
 [9 2 4]
 [5 2 4]
 [2 4 7]
 [7 9 1]
 [7 0 6]
 [9 9 7]
 [6 9 1]]
[0 0 1 0 0 1 3 3 2 1]
[0]
</pre>
</div>
</li></ol>
</li>
<li><a id="sec-2-2-2-2" name="sec-2-2-2-2"></a>空值<br  /><div class="outline-text-5" id="text-2-2-2-2">
<p>
空值使模型报错。<br  />
</p>
</div>
<ol class="org-ol"><li><a id="sec-2-2-2-2-1" name="sec-2-2-2-2-1"></a>丢弃<br  /></li>
<li><a id="sec-2-2-2-2-2" name="sec-2-2-2-2-2"></a>离散，填充众数<br  /></li>
<li><a id="sec-2-2-2-2-3" name="sec-2-2-2-2-3"></a>连续，填充均值<br  /></li>
<li><a id="sec-2-2-2-2-4" name="sec-2-2-2-2-4"></a>模型填充<br  /></li></ol>
</li></ol>
</div>
<div id="outline-container-sec-2-2-3" class="outline-4">
<h4 id="sec-2-2-3"><span class="section-number-4">2.2.3</span> 问题：模型调参无法提高</h4>
<div class="outline-text-4" id="text-2-2-3">
<p>
目的：简化模型，加速收敛，提高精度。<br  />
</p>
</div>
<ol class="org-ol"><li><a id="sec-2-2-3-1" name="sec-2-2-3-1"></a>规范化<br  /><ol class="org-ol"><li><a id="sec-2-2-3-1-1" name="sec-2-2-3-1-1"></a>standard<br  /><div class="outline-text-6" id="text-2-2-3-1-1">
<p>
适合整体不太规整，方差较大的场景。<br  />
</p>

<p>
$$
z =\frac{ x - \mu}{\sigma}
$$<br  />
\(\mu\) : mean of the sample<br  />
\(\sigma\) : standard deviation of the sample<br  />
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">from</span> sklearn.preprocessing <span style="color: #fff59d;">import</span> StandardScaler
<span style="color: #ffcc80;">data</span> = [[0, 0], [0, 0], [1, 1], [1, 1]]
<span style="color: #fff59d;">print</span>(data)
<span style="color: #ffcc80;">scaler</span> = StandardScaler()
scaler.fit(data)
<span style="color: #fff59d;">print</span>(scaler.transform(data))
</pre>
</div>
</div>
</li>
<li><a id="sec-2-2-3-1-2" name="sec-2-2-3-1-2"></a>minmax<br  /><div class="outline-text-6" id="text-2-2-3-1-2">
<p>
适合对存在极端大和小的点的数据。<br  />
</p>

\begin{equation}
X_{std} = \frac{X - X.min(axis=0)}{X.max(axis=0) - X.min(axis=0)} \\
X_{scaled} = X_{std} \cdot (max -min) + min
\end{equation}
<p>
max, min 是要缩放到的区域；<br  />
X.max, X.min 是样本某特征的最大值和最小值。<br  />
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">from</span> sklearn.preprocessing <span style="color: #fff59d;">import</span> MinMaxScaler

<span style="color: #ffcc80;">data</span> = [[-1, 2], [0, 6]]
<span style="color: #ffcc80;">scaler</span> = MinMaxScaler()
<span style="color: #fff59d;">print</span>(scaler.fit(data))
<span style="color: #fff59d;">print</span>(scaler.transform(data))
</pre>
</div>
</div>
</li>

<li><a id="sec-2-2-3-1-3" name="sec-2-2-3-1-3"></a>normalize<br  /><div class="outline-text-6" id="text-2-2-3-1-3">
<p>
服务于 大量 向量点乘运算的场景，防止因为向量值过大造成极端的影响。<br  />
主要思想：<br  />
对每个样本求其p范数，然后对该样本中每个元素除以该范数。(l1,l2范数）<br  />
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">from</span> sklearn.preprocessing <span style="color: #fff59d;">import</span> Normalizer
<span style="color: #ffcc80;">data</span> = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
<span style="color: #ffcc80;">scaler</span> = Normalizer()
<span style="color: #fff59d;">print</span>(scaler.fit(data))
<span style="color: #fff59d;">print</span>(scaler.transform(data))
<span style="color: #fff59d;">print</span>(scaler.transform([[2, 2]]))
</pre>
</div>
</div>
</li>

<li><a id="sec-2-2-3-1-4" name="sec-2-2-3-1-4"></a>binarizer<br  /><div class="outline-text-6" id="text-2-2-3-1-4">
<p>
对于某些定量特征，包含的有效信息为区间划分，<br  />
例如只关心学习成绩的及格和不及格。<br  />
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">from</span> sklearn.preprocessing <span style="color: #fff59d;">import</span> Binarizer

<span style="color: #ffcc80;">X</span> = [[ 1., -1.,  2.],[ 2.,  0.,  0.],[ 0.,  1., -1.]]
<span style="color: #ffcc80;">binarizer</span> = Binarizer().fit(X)  <span style="color: #b0bec5;"># </span><span style="color: #b0bec5;">This method is just there to implement the usual API and hence work in pipelines.</span>
<span style="color: #fff59d;">print</span>(binarizer.transform(X))
</pre>
</div>
</div>
</li>
<li><a id="sec-2-2-3-1-5" name="sec-2-2-3-1-5"></a>bucket<br  /><div class="outline-text-6" id="text-2-2-3-1-5">
<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">import</span> numpy <span style="color: #fff59d;">as</span> np
<span style="color: #fff59d;">import</span> pandas <span style="color: #fff59d;">as</span> pd

<span style="color: #ffcc80;">x</span> = np.random.randint(0, 100, 100)
<span style="color: #ffcc80;">boundaries</span> = [0, 60, 70, 80, 90, 100]
<span style="color: #b0bec5;"># </span><span style="color: #b0bec5;">names = ['pass', 'd', 'c', 'b', 'a']</span>
<span style="color: #ffcc80;">names</span> = [1, 2, 3, 4, 5]

<span style="color: #ffcc80;">score_bucket</span> = pd.cut(x, bins=boundaries, labels=names, right=<span style="color: #8bc34a;">False</span>)
<span style="color: #fff59d;">print</span>(<span style="color: #ff8A65;">type</span>(score_bucket))  <span style="color: #b0bec5;"># </span><span style="color: #b0bec5;">&lt;class 'pandas.core.arrays.categorical.Categorical'&gt;</span>
<span style="color: #fff59d;">print</span>(score_bucket.tolist())
<span style="color: #fff59d;">print</span>(score_bucket.size)
</pre>
</div>
</div>
</li></ol>
</li>

<li><a id="sec-2-2-3-2" name="sec-2-2-3-2"></a>异常的点<br  /><ol class="org-ol"><li><a id="sec-2-2-3-2-1" name="sec-2-2-3-2-1"></a>丢弃<br  /></li>
<li><a id="sec-2-2-3-2-2" name="sec-2-2-3-2-2"></a>规范化<br  /></li></ol>
</li></ol>
</div>
<div id="outline-container-sec-2-2-4" class="outline-4">
<h4 id="sec-2-2-4"><span class="section-number-4">2.2.4</span> 问题：样本不平衡</h4>
<div class="outline-text-4" id="text-2-2-4">
</div><ol class="org-ol"><li><a id="sec-2-2-4-1" name="sec-2-2-4-1"></a>多的采少<br  /></li>
<li><a id="sec-2-2-4-2" name="sec-2-2-4-2"></a>少的增多<br  /></li></ol>
</div>
<div id="outline-container-sec-2-2-5" class="outline-4">
<h4 id="sec-2-2-5"><span class="section-number-4">2.2.5</span> 问题：过拟合</h4>
<div class="outline-text-4" id="text-2-2-5">
<p>
目的：简化模型，提高精度。<br  />
</p>
</div>
<ol class="org-ol"><li><a id="sec-2-2-5-1" name="sec-2-2-5-1"></a>特征选择<br  /><ol class="org-ol"><li><a id="sec-2-2-5-1-1" name="sec-2-2-5-1-1"></a>filter<br  /><ol class="org-ol"><li><a id="sec-2-2-5-1-1-1" name="sec-2-2-5-1-1-1"></a>chi2<br  /><div class="outline-text-7" id="text-2-2-5-1-1-1">
<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">from</span> sklearn.feature_selection <span style="color: #fff59d;">import</span> SelectKBest
<span style="color: #fff59d;">from</span> sklearn.feature_selection <span style="color: #fff59d;">import</span> chi2
<span style="color: #fff59d;">from</span> sklearn.datasets <span style="color: #fff59d;">import</span> load_iris

<span style="color: #ffcc80;">iris</span> = load_iris()
<span style="color: #fff59d;">print</span>(iris.data[:3, :])
<span style="color: #ffcc80;">selector</span> = SelectKBest(chi2, k=2).fit(iris.data, iris.target)
<span style="color: #ffcc80;">data</span> = selector.transform(iris.data)
<span style="color: #fff59d;">print</span>(data[:3, :])
<span style="color: #fff59d;">print</span>(selector.scores_)
</pre>
</div>

<p>
输出结果：<br  />
</p>
<pre class="example">
[[5.1 3.5 1.4 0.2]
 [4.9 3.  1.4 0.2]
 [4.7 3.2 1.3 0.2]]
[[1.4 0.2]
 [1.4 0.2]
 [1.3 0.2]]
[ 10.81782088   3.7107283  116.31261309  67.0483602 ]
</pre>
</div>
</li>
<li><a id="sec-2-2-5-1-1-2" name="sec-2-2-5-1-1-2"></a>variance threshold<br  /><div class="outline-text-7" id="text-2-2-5-1-1-2">
<p>
先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。<br  />
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">from</span> sklearn.feature_selection <span style="color: #fff59d;">import</span> VarianceThreshold
<span style="color: #fff59d;">from</span> sklearn.datasets <span style="color: #fff59d;">import</span> load_iris

<span style="color: #ffcc80;">iris</span> = load_iris()
<span style="color: #fff59d;">print</span>(iris.data[0:5])
<span style="color: #ffcc80;">selector</span> = VarianceThreshold(threshold=.5).fit(iris.data, iris.target)
<span style="color: #ffcc80;">data</span> = selector.transform(iris.data)
<span style="color: #fff59d;">print</span>(data[0:5])
<span style="color: #fff59d;">print</span>(selector.variances_)
</pre>
</div>
<p>
输出：<br  />
</p>

<pre class="example">
[[5.1 3.5 1.4 0.2]
 [4.9 3.  1.4 0.2]
 [4.7 3.2 1.3 0.2]
 [4.6 3.1 1.5 0.2]
 [5.  3.6 1.4 0.2]]
[[5.1 1.4 0.2]
 [4.9 1.4 0.2]
 [4.7 1.3 0.2]
 [4.6 1.5 0.2]
 [5.  1.4 0.2]]
[0.68112222 0.18871289 3.09550267 0.57713289]
</pre>
</div>
</li></ol>
</li>
<li><a id="sec-2-2-5-1-2" name="sec-2-2-5-1-2"></a>wrapper<br  /><ol class="org-ol"><li><a id="sec-2-2-5-1-2-1" name="sec-2-2-5-1-2-1"></a>RFE<br  /><div class="outline-text-7" id="text-2-2-5-1-2-1">
<p>
RFE: recursive feature eliminate<br  />
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">from</span> sklearn.feature_selection <span style="color: #fff59d;">import</span> RFE
<span style="color: #fff59d;">from</span> sklearn.linear_model <span style="color: #fff59d;">import</span> LogisticRegression
<span style="color: #fff59d;">from</span> sklearn.datasets <span style="color: #fff59d;">import</span> load_iris

<span style="color: #ffcc80;">iris</span> = load_iris()
<span style="color: #fff59d;">print</span>(iris.data[0:5])
<span style="color: #ffcc80;">selector</span> = RFE(estimator=LogisticRegression(), n_features_to_select=2).fit(iris.data, iris.target)
<span style="color: #ffcc80;">data</span> = selector.transform(iris.data)
<span style="color: #fff59d;">print</span>(data[0:5])
<span style="color: #fff59d;">print</span>(selector.ranking_)
</pre>
</div>

<p>
输出:<br  />
</p>

<pre class="example">
[[5.1 3.5 1.4 0.2]
 [4.9 3.  1.4 0.2]
 [4.7 3.2 1.3 0.2]
 [4.6 3.1 1.5 0.2]
 [5.  3.6 1.4 0.2]]
[[3.5 0.2]
 [3.  0.2]
 [3.2 0.2]
 [3.1 0.2]
 [3.6 0.2]]
[3 1 2 1]
</pre>
</div>
</li></ol>
</li>
<li><a id="sec-2-2-5-1-3" name="sec-2-2-5-1-3"></a>embedded<br  /><div class="outline-text-6" id="text-2-2-5-1-3">
<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">from</span> sklearn.feature_selection <span style="color: #fff59d;">import</span> SelectFromModel
<span style="color: #fff59d;">from</span> sklearn.ensemble <span style="color: #fff59d;">import</span> GradientBoostingClassifier 
<span style="color: #fff59d;">from</span> sklearn.datasets <span style="color: #fff59d;">import</span> load_iris

<span style="color: #ffcc80;">iris</span> = load_iris()
<span style="color: #ffcc80;">selector</span> = SelectFromModel(GradientBoostingClassifier()).fit(iris.data, iris.target)
<span style="color: #fff59d;">print</span>(iris.data[0:5])
<span style="color: #ffcc80;">data</span> = selector.transform(iris.data)
<span style="color: #fff59d;">print</span>(data[0:5])
<span style="color: #fff59d;">print</span>(selector.estimator_.feature_importances_)
</pre>
</div>
<p>
输出：<br  />
</p>

<pre class="example">
[[5.1 3.5 1.4 0.2]
 [4.9 3.  1.4 0.2]
 [4.7 3.2 1.3 0.2]
 [4.6 3.1 1.5 0.2]
 [5.  3.6 1.4 0.2]]
[[1.4 0.2]
 [1.4 0.2]
 [1.3 0.2]
 [1.5 0.2]
 [1.4 0.2]]
[0.00612427 0.01303259 0.25968049 0.72116265]
</pre>
</div>
</li>
<li><a id="sec-2-2-5-1-4" name="sec-2-2-5-1-4"></a>decision tree<br  /></li></ol>
</li>
<li><a id="sec-2-2-5-2" name="sec-2-2-5-2"></a>降维<br  /><ol class="org-ol"><li><a id="sec-2-2-5-2-1" name="sec-2-2-5-2-1"></a>PCA<br  /><div class="outline-text-6" id="text-2-2-5-2-1">
<p>
PCA: Principal Component Analysis<br  />
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">from</span> sklearn <span style="color: #fff59d;">import</span> datasets
<span style="color: #fff59d;">from</span> sklearn.decomposition <span style="color: #fff59d;">import</span> PCA

<span style="color: #ffcc80;">iris</span> = datasets.load_iris()
<span style="color: #fff59d;">print</span>(iris.data[:3, :])

<span style="color: #ffcc80;">X_reduced</span> = PCA(n_components=3).fit_transform(iris.data)
<span style="color: #fff59d;">print</span>(X_reduced[:3,:])
</pre>
</div>

<p>
输出：<br  />
</p>

<pre class="example">
[[5.1 3.5 1.4 0.2]
 [4.9 3.  1.4 0.2]
 [4.7 3.2 1.3 0.2]]
[[-2.68412563  0.31939725 -0.02791483]
 [-2.71414169 -0.17700123 -0.21046427]
 [-2.88899057 -0.14494943  0.01790026]]
</pre>
</div>
</li></ol>
</li></ol>
</div>

<div id="outline-container-sec-2-2-6" class="outline-4">
<h4 id="sec-2-2-6"><span class="section-number-4">2.2.6</span> 问题：欠拟合</h4>
<div class="outline-text-4" id="text-2-2-6">
<p>
目的：使模型复杂化，提高精度。<br  />
</p>
</div>
<ol class="org-ol"><li><a id="sec-2-2-6-1" name="sec-2-2-6-1"></a>特征扩展<br  /><ol class="org-ol"><li><a id="sec-2-2-6-1-1" name="sec-2-2-6-1-1"></a>onehot<br  /><div class="outline-text-6" id="text-2-2-6-1-1">
<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">import</span> numpy <span style="color: #fff59d;">as</span> np
<span style="color: #fff59d;">from</span> sklearn.preprocessing <span style="color: #fff59d;">import</span> OneHotEncoder

<span style="color: #ffcc80;">enc</span> = OneHotEncoder()
<span style="color: #ffcc80;">arr</span> = np.arange(12).reshape([4, 3])
<span style="color: #fff59d;">print</span>(arr)
enc.fit(arr)
<span style="color: #fff59d;">print</span>(enc.n_values_)
<span style="color: #fff59d;">print</span>(enc.feature_indices_)
<span style="color: #fff59d;">print</span>(enc.transform([[0, 1, 2]]).toarray())
</pre>
</div>

<p>
输出：<br  />
</p>

<pre class="example">
[[ 0  1  2]
 [ 3  4  5]
 [ 6  7  8]
 [ 9 10 11]]
[10 11 12]
[ 0 10 21 33]
[[1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]]
</pre>


<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">import</span> pandas <span style="color: #fff59d;">as</span> pd

<span style="color: #ffcc80;">df</span> = pd.DataFrame({<span style="color: #9ccc65;">'A'</span>: [<span style="color: #9ccc65;">'a'</span>, <span style="color: #9ccc65;">'b'</span>, <span style="color: #9ccc65;">'a'</span>], <span style="color: #9ccc65;">'B'</span>: [<span style="color: #9ccc65;">'b'</span>, <span style="color: #9ccc65;">'a'</span>, <span style="color: #9ccc65;">'c'</span>], <span style="color: #9ccc65;">'C'</span>: [1, 2, 3]})
<span style="color: #ffcc80;">df_dummies</span> = pd.get_dummies(df, prefix=[<span style="color: #9ccc65;">'col1'</span>, <span style="color: #9ccc65;">'col2'</span>])
<span style="color: #fff59d;">print</span>(df_dummies)
</pre>
</div>

<p>
output:<br  />
</p>
<pre class="example">
   C  col1_a  col1_b  col2_a  col2_b  col2_c
0  1       1       0       0       1       0
1  2       0       1       1       0       0
2  3       1       0       0       0       1
</pre>
</div>
</li>
<li><a id="sec-2-2-6-1-2" name="sec-2-2-6-1-2"></a>polynomial<br  /><div class="outline-text-6" id="text-2-2-6-1-2">
<p>
\(a,b -> 1, a, b, a^2, ab, b^2\)<br  />
</p>

<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">from</span> sklearn.preprocessing <span style="color: #fff59d;">import</span> PolynomialFeatures
<span style="color: #fff59d;">import</span> numpy <span style="color: #fff59d;">as</span> np

<span style="color: #ffcc80;">X</span> = np.arange(4).reshape(2, 2)
<span style="color: #fff59d;">print</span>(X)

<span style="color: #ffcc80;">poly</span> = PolynomialFeatures(2)
<span style="color: #fff59d;">print</span>(poly.fit_transform(X))
</pre>
</div>

<p>
output:<br  />
</p>

<pre class="example">
[[0 1]
 [2 3]]
[[1. 0. 1. 0. 0. 1.]
 [1. 2. 3. 4. 6. 9.]]
</pre>
</div>
</li></ol>
</li></ol>
</div>
</div>

<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3"><span class="section-number-3">2.3</span> 模型训练</h3>
<div class="outline-text-3" id="text-2-3">
</div><div id="outline-container-sec-2-3-1" class="outline-4">
<h4 id="sec-2-3-1"><span class="section-number-4">2.3.1</span> 数据观察</h4>
<div class="outline-text-4" id="text-2-3-1">
<p>
观察数据样本量，特征量，类型，是否有空值，最大最小值，均值等<br  />
</p>
<div class="org-src-container">

<pre class="src src-python">pd.set_option(<span style="color: #9ccc65;">'display.max_columns'</span>, <span style="color: #8bc34a;">None</span>)  <span style="color: #b0bec5;"># </span><span style="color: #b0bec5;">to show all the columns</span>
<span style="color: #ffcc80;">data</span> = pd.read_csv(<span style="color: #9ccc65;">'data.csv'</span>)
data.columns
data.info()
data.describe()
data[<span style="color: #9ccc65;">'column name'</span>].value_counts()
data.plot()
</pre>
</div>
</div>
</div>


<div id="outline-container-sec-2-3-2" class="outline-4">
<h4 id="sec-2-3-2"><span class="section-number-4">2.3.2</span> 模型选择</h4>
<div class="outline-text-4" id="text-2-3-2">
<p>
可以使用LR模型建立baseline，再尝试不同算法。<br  />
准确度依次下降的如下算法：<br  />
</p>
</div>

<ol class="org-ol"><li><a id="sec-2-3-2-1" name="sec-2-3-2-1"></a>神经网络<br  /></li>
<li><a id="sec-2-3-2-2" name="sec-2-3-2-2"></a>集成学习<br  /><div class="outline-text-5" id="text-2-3-2-2">
<ul class="org-ul">
<li>随机森林<br  />
</li>
<li>GBDT<br  />
</li>
<li>AdaBoost<br  />
</li>
</ul>
</div>
</li>
<li><a id="sec-2-3-2-3" name="sec-2-3-2-3"></a>简单模型<br  /><div class="outline-text-5" id="text-2-3-2-3">
<ul class="org-ul">
<li>Linear Regression<br  />
</li>
<li>SVM<br  />
</li>
<li>Decision Tree<br  />
</li>
</ul>
</div>
</li></ol>
</div>
<div id="outline-container-sec-2-3-3" class="outline-4">
<h4 id="sec-2-3-3"><span class="section-number-4">2.3.3</span> 参数优化</h4>
</div>
</div>
<div id="outline-container-sec-2-4" class="outline-3">
<h3 id="sec-2-4"><span class="section-number-3">2.4</span> 使用模型</h3>
<div class="outline-text-3" id="text-2-4">
</div><div id="outline-container-sec-2-4-1" class="outline-4">
<h4 id="sec-2-4-1"><span class="section-number-4">2.4.1</span> sklearn</h4>
<div class="outline-text-4" id="text-2-4-1">
<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">from</span> sklearn.linear_model <span style="color: #fff59d;">import</span> LogisticRegression
<span style="color: #fff59d;">from</span> sklearn.externals <span style="color: #fff59d;">import</span> joblib

<span style="color: #ffcc80;">lr</span> = LogisticRegression()
<span style="color: #b0bec5;"># </span><span style="color: #b0bec5;">train...</span>

<span style="color: #b0bec5;"># </span><span style="color: #b0bec5;">save</span>
joblib.dump(lr, <span style="color: #9ccc65;">'lr.model'</span>)

<span style="color: #b0bec5;"># </span><span style="color: #b0bec5;">load</span>
<span style="color: #ffcc80;">lr_load</span> = joblib.load(<span style="color: #9ccc65;">'lr.model'</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-sec-2-4-2" class="outline-4">
<h4 id="sec-2-4-2"><span class="section-number-4">2.4.2</span> tensorflow</h4>
<div class="outline-text-4" id="text-2-4-2">
<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">import</span> tensorflow <span style="color: #fff59d;">as</span> tf

<span style="color: #ffcc80;">sess</span> = tf.Session()
<span style="color: #b0bec5;"># </span><span style="color: #b0bec5;">train...</span>

<span style="color: #b0bec5;"># </span><span style="color: #b0bec5;">save</span>
<span style="color: #ffcc80;">saver</span> = tf.train.saver()
saver.save(sess, <span style="color: #9ccc65;">'model_path'</span>)

<span style="color: #b0bec5;"># </span><span style="color: #b0bec5;">load</span>
saver.restore(sess, <span style="color: #9ccc65;">'model_path'</span>))
</pre>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> 统计学习</h2>
<div class="outline-text-2" id="text-3">
</div><div id="outline-container-sec-3-1" class="outline-3">
<h3 id="sec-3-1"><span class="section-number-3">3.1</span> 基于概率的信任</h3>
<div class="outline-text-3" id="text-3-1">
<p>
假设抛硬币，抛了一次，正面向上，不要信任正面向上的概率为1，而是信任，以一定概率，正面向上的概率为1；<br  />
抛了两次硬币，一正一反，不要信任正面向上的概率为0.5，而是信任，以一定的概率，正面向上的概率为0.5；<br  />
</p>

<p>
量化为如下公式(Hoeffding Inequality)：<br  />
$$
P(|v - \mu| > \epsilon) \le 2exp \left (-2\epsilon^2N \right )
$$<br  />
</p>

<p>
\(v\) 为的统计值；<br  />
\(\mu\) 为真实值；<br  />
\(N\) 为样本量；<br  />
\(\epsilon\) 为误差；<br  />
</p>
</div>
</div>

<div id="outline-container-sec-3-2" class="outline-3">
<h3 id="sec-3-2"><span class="section-number-3">3.2</span> 为什么用统计学？</h3>
<div class="outline-text-3" id="text-3-2">
<p>
由基于概率的信任可知，我们得到的规律不一定是正确的，而已以一定概率正确而已。<br  />
</p>

<p>
由Hoeffding Inequality可得大数定律：<br  />
当试验次数足够多时，事件出现的频率无限接近于该事件发生的概率。<br  />
</p>

<p>
所以应用统计学，使我们的结论更接近客观世界的规律。<br  />
</p>
</div>
</div>

<div id="outline-container-sec-3-3" class="outline-3">
<h3 id="sec-3-3"><span class="section-number-3">3.3</span> 什么是学习？</h3>
<div class="outline-text-3" id="text-3-3">
<p>
Herbet Simon:<br  />
如果一个系统能通过执行某个过程改进它的性能，这就是学习。<br  />
</p>
</div>
</div>

<div id="outline-container-sec-3-4" class="outline-3">
<h3 id="sec-3-4"><span class="section-number-3">3.4</span> 什么是统计学习？</h3>
<div class="outline-text-3" id="text-3-4">
<p>
基于数据，构建统计模型，并运用模型对数据进行预测与分析。<br  />
</p>
</div>
</div>

<div id="outline-container-sec-3-5" class="outline-3">
<h3 id="sec-3-5"><span class="section-number-3">3.5</span> 统计学习三要素</h3>
<div class="outline-text-3" id="text-3-5">
</div><div id="outline-container-sec-3-5-1" class="outline-4">
<h4 id="sec-3-5-1"><span class="section-number-4">3.5.1</span> 假设空间</h4>
<div class="outline-text-4" id="text-3-5-1">
<p>
映射关系的集合。也可称之为模型。比如：<br  />
</p>
<ul class="org-ul">
<li>神经网络<br  />
</li>
<li>随机森林<br  />
</li>
<li>SVM<br  />
</li>
<li>线性回归<br  />
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-3-5-2" class="outline-4">
<h4 id="sec-3-5-2"><span class="section-number-4">3.5.2</span> 优化目标</h4>
<div class="outline-text-4" id="text-3-5-2">
<p>
误差的衡量。<br  />
</p>
</div>
<ol class="org-ol"><li><a id="sec-3-5-2-1" name="sec-3-5-2-1"></a>损失函数<br  /><div class="outline-text-5" id="text-3-5-2-1">
<p>
监督学习是在假设空间中选择模型$f$，对于给定的输入\(X\) ，给出相应的预测输出\(f(X)\) 。<br  />
预测值\(f(X)\) 和真实值$Y$可能一直也可能不一致，即有所损失，用损失函数来量化损失。<br  />
</p>

<p>
常用损失函数：<br  />
0-1损失函数：<br  />
$$
L(Y,f(X)) = \begin{cases}
1, & Y \ne f(X) \\
0, & Y = f(X)
\end{cases}
$$<br  />
平方损失函数：<br  />
$$
L(Y,f(X)) = (Y-f(X))^2
$$<br  />
绝对损失函数：<br  />
$$
L(Y,f(X)) = |Y-f(X))|
$$<br  />
对数损失函数：<br  />
$$
L(Y,f(X)) = -log P(Y|X)
$$<br  />
</p>
</div>
</li>

<li><a id="sec-3-5-2-2" name="sec-3-5-2-2"></a>经验损失函数<br  /><div class="outline-text-5" id="text-3-5-2-2">
<p>
给定数据集：<br  />
$$ T = \{(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\} $$<br  />
模型\(f(X)\) 关于训练数据集的平均损失称为经验损失(empirical loss)：<br  />
$$ R_{emp}(f) = \frac{1}{n} \sum_{i=1}^n L(y_i,f(x_i)) $$<br  />
</p>

<p>
期望损失\(R_{exp}(f)\) 是模型关于XY联合分布的期望损失；经验损失\(R_{emp}(f)\) 是模型关于训练样本的平均损失。<br  />
根据大数定律，当样本容量趋于无穷大时，经验损失等于期望损失。<br  />
</p>
</div>
</li>

<li><a id="sec-3-5-2-3" name="sec-3-5-2-3"></a>结构损失函数<br  /><div class="outline-text-5" id="text-3-5-2-3">
<p>
以\(R_{emp}(f)\) 来代替\(R_{exp}(f)\) 有一个问题，就是样本容量，<br  />
在样本容量较小的情况下，容易产生过拟合现象。<br  />
</p>

<p>
结构损失函数就是在经验损失函数上加正则化项，<br  />
即，本样本无法代表规律本身，而是对规律本身的一个修正。（频率学派和贝叶斯学派）<br  />
</p>
\begin{equation}
R_{reg}(f) = \frac{1}{n} \sum_{i=1}^n L(y_i,f(x_i)) + \lambda J(f) 
\end{equation}
</div>
</li></ol>
</div>

<div id="outline-container-sec-3-5-3" class="outline-4">
<h4 id="sec-3-5-3"><span class="section-number-4">3.5.3</span> 求解算法</h4>
<div class="outline-text-4" id="text-3-5-3">
<p>
如何使优化目标求得最优值。<br  />
</p>
<ul class="org-ul">
<li>梯度下降法<br  />
</li>
<li>牛顿法<br  />
</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4"><span class="section-number-2">4</span> Algorithms</h2>
<div class="outline-text-2" id="text-4">
</div><div id="outline-container-sec-4-1" class="outline-3">
<h3 id="sec-4-1"><span class="section-number-3">4.1</span> LMS (least mean square)</h3>
<div class="outline-text-3" id="text-4-1">
<p>
特点：使用均方差作为判断依据。（损失函数）<br  />
$$
x为输入矩阵，\theta 为权重矩阵，h_\theta (x)预测输出，y为真实输出，J(\theta) 为损失函数，
\alpha 为学习率
$$<br  />
</p>

<p>
$$
J(\theta) = \frac{1}{2}(h_\theta(x) - y)^2
$$<br  />
</p>

<p>
梯度下降法求解最优解。<br  />
</p>

<p>
$$
\theta_j = \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)
$$<br  />
</p>

<p>
$$
\frac{\partial}{\partial\theta_j}J(\theta) = \frac{\partial}{\partial\theta_j} \frac{1}{2}(h_\theta(x) - y)^2 \\
= (h_\theta(x) - y)\frac{\partial}{\partial\theta_j}(h_\theta(x) - y) \\
= (h_\theta(x) - y)\frac{\partial}{\partial\theta_j}(\sum_\limits{i=1}^{n}\theta_ix_i-y) \\
= (h_\theta(x) - y)x_j
$$<br  />
</p>

<p>
对于单个样本i<br  />
$$
\theta_j = \theta_j + \alpha(y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)}
$$<br  />
</p>
</div>
</div>

<div id="outline-container-sec-4-2" class="outline-3">
<h3 id="sec-4-2"><span class="section-number-3">4.2</span> KNN</h3>
<div class="outline-text-3" id="text-4-2">
<p>
k-nearest neighbors<br  />
核心思想：在未知目标特性的前提下，通过其周围事物的特性来作为判别目标特性的标准。<br  />
</p>
</div>

<div id="outline-container-sec-4-2-1" class="outline-4">
<h4 id="sec-4-2-1"><span class="section-number-4">4.2.1</span> 三个要素：</h4>
<div class="outline-text-4" id="text-4-2-1">
<ol class="org-ol">
<li>k值的选择，即多少个事物作为判断标准（过小容易过拟合，过大容易欠拟合）<br  />
</li>
<li>距离的度量，即选用什么标准来度量距离<br  />
</li>
<li>决策规则，即在最近临事物特性确定的情况下，怎么确定目标特性。（分类，一般使用多数投票；回归，一般使用均值）<br  />
</li>
</ol>
</div>
</div>

<div id="outline-container-sec-4-2-2" class="outline-4">
<h4 id="sec-4-2-2"><span class="section-number-4">4.2.2</span> 实现：</h4>
<div class="outline-text-4" id="text-4-2-2">
</div><ol class="org-ol"><li><a id="sec-4-2-2-1" name="sec-4-2-2-1"></a>暴力搜索<br  /><div class="outline-text-5" id="text-4-2-2-1">
<p>
sklearn使用这个实现的<br  />
</p>
</div>
</li>
<li><a id="sec-4-2-2-2" name="sec-4-2-2-2"></a>KD树(k-dimention)<br  /><div class="outline-text-5" id="text-4-2-2-2">
<p>
KD树，是在欧几里得空间点的结构。是一种空间二分树。<br  />
</p>

<p>
在开始对测试进行分类前，先建立模型KD树。<br  />
例子：<br  />
二维样本6个，{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}。<br  />
KD书的建立：<br  />
</p>
<ol class="org-ol">
<li>选取特征方差最大的k作为根节点<br  />
</li>
<li>特征k中的中位数middle作为划分点，小于middle的进入左子树，大于等于middle的化入右子树（或者轮流选取坐标轴切分）<br  />
</li>
<li>重复1-2<br  />
</li>
</ol>
<p>
<img src="pics/kd-build1.jpg" alt="kd-build1.jpg" /><br  />
<img src="pics/kd-build2.jpg" alt="kd-build2.jpg" /><br  />
KD树搜索：<br  />
如查找点为（2，4.5）。先进行二叉查找，先从（7,2）查找到（5,4）节点，在进行查找时是由y = 4为分割超平面的，<br  />
由于查找点为y值为4.5，因此进入右子空间查找到（4,7），形成搜索路径&lt;（7,2），（5,4），（4,7）&gt;，<br  />
取（4,7）为当前最近邻点，计算其与目标查找点的距离为3.202。然后回溯到（5,4），计算其与查找点之间的距离为3.041。<br  />
以（2，4.5）为圆心，以3.041为半径作圆。可见该圆和y = 4超平面交割，所以需要进入（5,4）左子空间进行查找。<br  />
此时需将（2,3）节点加入搜索路径中得&lt;（7,2），（2,3）&gt;。<br  />
回溯至（2,3）叶子节点，（2,3）距离（2,4.5）比（5,4）要近，所以最近邻点更新为（2，3），最近距离更新为1.5。<br  />
回溯至（7,2），以（2,4.5）为圆心1.5为半径作圆，并不和x = 7分割超平面交割。<br  />
至此，搜索路径回溯完。返回最近邻点（2,3），最近距离1.5。<br  />
<img src="pics/kd-search1.jpg" alt="kd-search1.jpg" /><br  />
<img src="pics/kd-search2.jpg" alt="kd-search2.jpg" /><br  />
</p>
</div>
</li>
<li><a id="sec-4-2-2-3" name="sec-4-2-2-3"></a>球树<br  /></li></ol>
</div>
<div id="outline-container-sec-4-2-3" class="outline-4">
<h4 id="sec-4-2-3"><span class="section-number-4">4.2.3</span> 优缺点</h4>
<div class="outline-text-4" id="text-4-2-3">
<p>
优点：<br  />
</p>
<ol class="org-ol">
<li>既可以用来做分类也可以用来做回归<br  />
</li>
<li>简单易于理解<br  />
</li>
<li>新数据可以直接加入数据集而不必进行重新训练<br  />
</li>
</ol>
<p>
缺点：<br  />
</p>
<ol class="org-ol">
<li>向量的维度越高，欧式距离的区分能力就越弱<br  />
</li>
<li>样本平衡度依赖高<br  />
</li>
<li>向量的维度越高，欧式距离的区分能力就越弱<br  />
</li>
<li>不适合大样本量（计算距离花费时间较长）<br  />
</li>
</ol>
</div>
</div>
</div>


<div id="outline-container-sec-4-3" class="outline-3">
<h3 id="sec-4-3"><span class="section-number-3">4.3</span> Naive Bayes （朴素贝叶斯）</h3>
<div class="outline-text-3" id="text-4-3">
</div><div id="outline-container-sec-4-3-1" class="outline-4">
<h4 id="sec-4-3-1"><span class="section-number-4">4.3.1</span> 理论推导</h4>
<div class="outline-text-4" id="text-4-3-1">
<p>
bayes公式：<br  />
</p>
\begin{equation}
P(AB)=P(A)\cdot P(B|A) = P(B)\cdot P(A|B)
\end{equation}
<p>
用来描述两个条件概率之间的关系。<br  />
即当你无法判断一个事物的的本质的时候，可以依靠与该事物相关的事件来判断该事物本质属性的概率。<br  />
用数学表达就是：支持某项属性的事件发生的越多，则该属性成立的可能性就越大。<br  />
</p>

<p>
bayes公式的变体：<br  />
</p>
\begin{equation}
P(B|A) = \frac{P(A|B) \cdot P(B)}{P(A)}
\end{equation}


<p>
多特征值的变体：（符号无关性质得出）<br  />
</p>
\begin{equation}
P(B|A_{1}\cdots A_{n}) = \frac{P(A_{1}\cdots A_{n}|B) \cdot P(B)}{P(A_{1}\cdots A_{n})}
\end{equation}

<p>
naive（朴素）的意思是假设A<sub>i</sub>之间相互独立，则得到<br  />
</p>
\begin{equation}
P(B|A_{1}\cdots A_{n}) = \frac{P(A_{1}|B)\cdots P(A_{n}|B) \cdot P(B)}{P(A_{1}\cdots A_{n})}
\end{equation}

<p>
进一步以机器学习的数据推导贝叶斯分类器：<br  />
假设A有n个属性，B有m个类别，贝叶斯分类器就是计算出概率最大的那个分类。<br  />
$$
P(B_{1}|A_{1}\cdots A_{n}) = \frac{P(A_{1}|B_{1})\cdots P(A_{n}|B_{1}) \cdot P(B_{1})}{P(A_{1}\cdots A_{n})}
$$<br  />
$$\vdots$$<br  />
$$
P(B_{m}|A_{1}\cdots A_{n}) = \frac{P(A_{1}|B_{m})\cdots P(A_{n}|B_{m}) \cdot P(B_{m})}{P(A_{1}\cdots A_{n})}
$$<br  />
</p>

<p>
由于有相同的分母\(P(A_{1}\cdots A_{n})\),可以省去，则为：<br  />
$$
P(B_{1}|A_{1}\cdots A_{n}) = P(A_{1}|B_{1})\cdots P(A_{n}|B_{1}) \cdot P(B_{1})
$$<br  />
$$\vdots$$<br  />
$$
P(B_{m}|A_{1}\cdots A_{n}) = P(A_{1}|B_{m})\cdots P(A_{n}|B_{m}) \cdot P(B_{m})
$$<br  />
选取其中最大的概率为该分类，其中等式右边的概率可以根据样本计算出来。<br  />
</p>
</div>
</div>


<div id="outline-container-sec-4-3-2" class="outline-4">
<h4 id="sec-4-3-2"><span class="section-number-4">4.3.2</span> Gaussian Naive Bayes</h4>
<div class="outline-text-4" id="text-4-3-2">
<p>
在朴素贝叶斯分类器的基础上，假设A属性的分布符合高斯分布，简化了概率的计算，可以高斯公式来计算个属性的概率。<br  />
</p>

<p>
高斯分布：<br  />
$$
\frac{1}{\sqrt{2\pi\sigma^{2}}}exp({-\frac{(x-\mu)^2}{2\sigma^2}})
$$<br  />
其中，\(\mu\) 为均值， \(\sigma\) 为标准差。<br  />
</p>
</div>
</div>

<div id="outline-container-sec-4-3-3" class="outline-4">
<h4 id="sec-4-3-3"><span class="section-number-4">4.3.3</span> 优缺点</h4>
<div class="outline-text-4" id="text-4-3-3">
<p>
优点：<br  />
</p>
<ol class="org-ol">
<li>发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。<br  />
</li>
<li>是对小规模的数据表现很好，能个处理多分类任务，适合增量式训练（即可以实时的对新增的样本进行训练）<br  />
</li>
<li>对缺失数据不太敏感<br  />
</li>
</ol>


<p>
缺点：<br  />
</p>
<ol class="org-ol">
<li>由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。<br  />
</li>
<li>需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。<br  />
</li>
<li>分类决策存在错误率（？）<br  />
</li>
<li>对输入数据的表达形式很敏感（？）<br  />
</li>
</ol>
</div>
</div>


<div id="outline-container-sec-4-3-4" class="outline-4">
<h4 id="sec-4-3-4"><span class="section-number-4">4.3.4</span> 常用领域</h4>
<div class="outline-text-4" id="text-4-3-4">
<p>
分本分类<br  />
</p>
</div>
</div>
</div>
<div id="outline-container-sec-4-4" class="outline-3">
<h3 id="sec-4-4"><span class="section-number-3">4.4</span> SVM (support vector machine)</h3>
<div class="outline-text-3" id="text-4-4">
<p>
SVM是一种二分类模型。<br  />
基本模型是定义在特征空间上的间隔最大的线性分类器。<br  />
</p>

<p>
SVM构建由简至繁的模型：<br  />
</p>
<ul class="org-ul">
<li>线性可分支持向量机(linear support vector machine in linearly separatable case)<br  />
</li>
<li>线性支持向量机(linear support vector machine)<br  />
</li>
<li>非线性支持向量机(non-linear support vector machine)<br  />
</li>
</ul>
</div>


<div id="outline-container-sec-4-4-1" class="outline-4">
<h4 id="sec-4-4-1"><span class="section-number-4">4.4.1</span> 线性可分支持向量机</h4>
<div class="outline-text-4" id="text-4-4-1">
<p>
SVM是在特征空间上进行的。（将输入空间映射到特征空间）<br  />
</p>

<p>
假定给定特征空间上的训练数据集：<br  />
$$ T = \{ (x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n) \} $$<br  />
</p>

<p>
一般地，当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开。<br  />
</p>
</div>

<ol class="org-ol"><li><a id="sec-4-4-1-1" name="sec-4-4-1-1"></a>目标函数<br  /><div class="outline-text-5" id="text-4-4-1-1">
<p>
假设分离超平面为：<br  />
$$ w^Tx + b = 0 $$<br  />
对应的分类决策函数为：<br  />
$$ f(x) = sign(w^Tx + b)$$<br  />
其中sign为符号函数：<br  />
$$ 
sing(x) = \begin{cases}
+1, & x \ge 0 \\
-1, & x < 0
\end{cases}
$$<br  />
</p>
</div>
</li>

<li><a id="sec-4-4-1-2" name="sec-4-4-1-2"></a>损失函数<br  /><div class="outline-text-5" id="text-4-4-1-2">
<p>
一般来说，一点距离超平面距离的大小可表示分类预测的置信度。<br  />
距离越大，置信度越大。<br  />
间隔最大化意味着超平面以最大的置信度对训练数据进行了分类。<br  />
</p>

<p>
为了计算方便，规定法线方向的样本为正样本，分类为+1，<br  />
法线反方向的样本为负样本，分类为-1.<br  />
</p>

<p>
点\((x_i,y_i)\) 被超平面\((w,b)\) 分类时，点\(x_i\) 与超平面\((w,b)\) 的距离为：<br  />
$$ d_i = \frac{y_i(wx_i + b)}{||w||} $$<br  />
其中\(||w||\) 为\(l2\) 范数：\(\sqrt{w_1^2 + w_2^2 + \cdots + w_n^2}\) <br  />
当\((x_i,y_i)\) 被正确分类时，\(d_i\) 为正值，被错误分类时，\(d_i\) 为负值。<br  />
</p>

<hr  />
<p>
自己尝试了损失函数为:<br  />
$$ L(f) = - \sum_{i=1}^n \frac{y_i(wx_i+b)}{||w||} $$<br  />
其中负号是为了将求极大值转化为求极小值。<br  />
</p>

<p>
但这个损失函数存在着一个问题，就是在解的不唯一性，分离超平面的平移无法限制。<br  />
</p>
<hr  />

<p>
设\(d\) 为样本集中，样本点距离超平面的最小值：<br  />
$$
d = \min\limits_{i \in [1,n]}d_i
$$<br  />
</p>

<p>
求最大间隔超平面的问题变为如下约束最优化问题：<br  />
</p>
\begin{equation}
\label{svm}
L(f) = \max\limits_{w,b}d \\
s.t. \quad  d_i \ge d , \quad i \in [1,n]
\end{equation}


<p>
如果\(w,b\) 分别变为\(\lambda w,\lambda b\) ，带入d的距离公式，可知距离并不改变，<br  />
对目标函数的优化并没有影响，就是说，它产生了一个等价的最优化问题，<br  />
于是，取\(y_i(wx_j+b)=1\) ，其中\(j\) 表示d取最小值的样本点，则最优化问题\((\ref{svm})\) 变为:<br  />
</p>
\begin{equation}
\label{svm2}
L(f) = \max\limits_{w,b} \frac{1}{||w||} \\
s.t. \quad  \frac{y_i(wx_i+b)}{||w||} \ge \frac{1}{||w||} , \quad i \in [1,n]
\end{equation}

<p>
简化为：<br  />
</p>
\begin{equation}
\label{svm3}
L(f) = \max\limits_{w,b} \frac{1}{||w||} \\
s.t. \quad  y_i(wx_i+b) - 1 \ge 0 , \quad i \in [1,n]
\end{equation}

<p>
注意到最大化\(\frac{1}{||w||}\) 和最小化\(\frac{1}{2}||w||^2\) 是等价的，于是得到如下最优化问题：<br  />
</p>
\begin{equation}
\label{svm4}
L(f) = \min\limits_{w,b} \frac{1}{2}||w||^2 \\
s.t. \quad  y_i(wx_i+b) - 1 \ge 0 , \quad i \in [1,n]
\end{equation}

<p>
\((\ref{svm4})\) 为最终的损失函数。<br  />
</p>
</div>
</li>


<li><a id="sec-4-4-1-3" name="sec-4-4-1-3"></a>求解算法<br  /></li></ol>
</div>
</div>



<div id="outline-container-sec-4-5" class="outline-3">
<h3 id="sec-4-5"><span class="section-number-3">4.5</span> Decision Tree</h3>
<div class="outline-text-3" id="text-4-5">

<div class="figure">
<p><img src="pics/decision_tree.png" alt="decision_tree.png" /><br  />
</p>
</div>

<p>
从形状上看，是一颗倒着的树。<br  />
</p>

<p>
过程：<br  />
现在有三个属性，颜色，价格，大小，<br  />
每次选择一个特征进行分类，重复这个过程，直到分类完成。<br  />
</p>

<p>
所以，决策树最重要的就是首先以那个属性划分，再以哪个属性划分，划分的结果最好。<br  />
这就需要对划分结果进行量化。<br  />
</p>

<p>
常见量化划分结果的好坏的有：ID3, C4.5, CART.<br  />
</p>

<p>
假设有数据集D，D中数据属于k个类别。<br  />
用于下面公式的推导。<br  />
</p>
</div>

<div id="outline-container-sec-4-5-1" class="outline-4">
<h4 id="sec-4-5-1"><span class="section-number-4">4.5.1</span> id3</h4>
<div class="outline-text-4" id="text-4-5-1">
<p>
measure: information gain<br  />
</p>

<p>
第1步： 计算数据集D的信息熵<br  />
$$
H(D) = -\sum_{i=1}^{k} P(i) log_{2}P(i)
$$<br  />
其中，\(P(i)\) 为数据i类的概率。<br  />
</p>

<p>
第2步： 假设以A特征进行分类，分为n个子数据集，计算分类后数据集的信息熵<br  />
$$
H(D|A) = -\sum_{i=1}^{n} \frac{|D_i|}{|D|} H(D_i)
$$<br  />
其中，D所有样本个数，\(D_{i}\) 为第i个自己样本个数。<br  />
添加系数\(\frac{D_i}{D}\) 是为了消除样本数量不一致。<br  />
</p>

<p>
第3步： 差值为信息增益<br  />
$$
g(D,A) = H(D) - H(D|A)
$$<br  />
</p>


<p>
缺点：通过计算公式我们可知，如果某个特征的值比较多，那么以这个特征划分后的信息增益一般会比较大。<br  />
</p>
</div>
</div>

<div id="outline-container-sec-4-5-2" class="outline-4">
<h4 id="sec-4-5-2"><span class="section-number-4">4.5.2</span> c4.5</h4>
<div class="outline-text-4" id="text-4-5-2">
<p>
solve the problem of id3<br  />
同id3相比，第1，2步不变，第3步以信息增益率来衡量<br  />
首先增加惩罚项，即如果你的特征中值比较多，那么除的数也比较大。<br  />
</p>

<p>
惩罚项：<br  />
$$
penalty(D,A) = \sum_{i=1}^{n} \frac{|D_i|}{|D|} log_{2}\frac{|D_i|}{|D|}
$$<br  />
</p>

<p>
信息增益率：<br  />
$$
gr(D,A) = \frac{g(D,A)}{penalty(D,A)}
$$<br  />
</p>
</div>
</div>

<div id="outline-container-sec-4-5-3" class="outline-4">
<h4 id="sec-4-5-3"><span class="section-number-4">4.5.3</span> cart</h4>
<div class="outline-text-4" id="text-4-5-3">
<p>
cart: Classification And Regression Tree<br  />
从名字上可以看出，以CART为度量来生成的决策树是支持回归问题的。<br  />
</p>


<p>
cart使用Gini来度量划分结果的好坏。<br  />
第1步： 计算数据集D的Gini系数<br  />
$$
Gini(D) = \sum_{i=1}^{k} P(i)(1-P(i)) = 1- \sum_{i=1}^{k} P(i)^2
$$<br  />
</p>

<p>
第2步： 计算数据集以A特征进行划分后的Gini系数<br  />
$$
Gini(D|A) = \sum_{i=1}^{n} \frac{|D_i|}{|D|} Gini(D_i)
$$<br  />
</p>

<p>
第三部： 计算Gini差值<br  />
由于CART可以处理离散和连续特征值问题，计算的方式有所区别。<br  />
离散： 直接计算就可以。<br  />
$$
g(D,A) = Gini(D) - Gini(D|A)
$$<br  />
</p>

<p>
连续：<br  />
因为连续属性的可取值数目不再有限，因此不能像前面处理离散属性枚举离散属性取值来对结点进行划分。因此需要连续属性离散化。<br  />
常用的离散化策略是二分法，下面以二分法对连续问题离散化。<br  />
</p>

<p>
在给定数据集D上有连续属性A，假设a在D上出现了n个不同的取值，<br  />
先把这些值从小到达排序，记为：\({a_1 , a_2 , \cdots , a_n}\) ，<br  />
基于切分点t可将数据集D分为\(D_{t}^{-}\) 和\(D_{t}^{+}\) ，<br  />
其中，\(D_{t}^{-}\) 是包含那些在A属性上取值不大于t的样本，<br  />
\(D_{t}^{+}\) 为包含那些在a属性上取值大于t的样本。<br  />
显然，对于相邻的属性取值\(a^i\) 和\(a^{i+1}\) 来说，<br  />
t在区间\([a^i , a^{i+1})\) 中任意取值所产生的划分结果相同。<br  />
以取中值为例，对连续属性a，可以考察包含n-1个元素的候选划分点的集合<br  />
$$
T_a = \left \{ \frac{a^i + a^{i+1}}{2} , \ \ 1 \le i \le n-1 \right \}
$$<br  />
</p>

<p>
然后，就像处理离散属性值那样来考虑这些切分点，选择最优的划分点进行样本集合的划分。<br  />
</p>

<p>
$$
g(D,A) = \max_{t \in T_a} g(D,A,t) = \max_{t \in T_a} \left ( Gini(D) - \sum_{\lambda \in \{-,+\}}  \frac{|D_{t}^{\lambda}|}{|D|} Gini(D_{t}^{\lambda}) \right )
$$<br  />
</p>
</div>
</div>


<div id="outline-container-sec-4-5-4" class="outline-4">
<h4 id="sec-4-5-4"><span class="section-number-4">4.5.4</span> 剪枝</h4>
<div class="outline-text-4" id="text-4-5-4">
<p>
放任生长容易造成过拟合<br  />
</p>
</div>

<ol class="org-ol"><li><a id="sec-4-5-4-1" name="sec-4-5-4-1"></a>预剪枝<br  /><div class="outline-text-5" id="text-4-5-4-1">
<p>
熵增量小于阈值，停止生长<br  />
控制树的高度<br  />
控制叶子的数量<br  />
</p>
</div>
</li>


<li><a id="sec-4-5-4-2" name="sec-4-5-4-2"></a>后剪枝<br  /><div class="outline-text-5" id="text-4-5-4-2">
<p>
剪枝的过程是对拥有同样父节点的一组节点进行检查，判断如果将其合并，熵的增加 量是否小于某一阈值。<br  />
如果满足阈值要求，则这一组节点可以合并一个节点，其中包含了所有可能的结果。<br  />
</p>
</div>
</li></ol>
</div>


<div id="outline-container-sec-4-5-5" class="outline-4">
<h4 id="sec-4-5-5"><span class="section-number-4">4.5.5</span> 损失函数</h4>
<div class="outline-text-4" id="text-4-5-5">
<p>
所有叶节点交叉熵的累加<br  />
</p>
</div>
</div>

<div id="outline-container-sec-4-5-6" class="outline-4">
<h4 id="sec-4-5-6"><span class="section-number-4">4.5.6</span> 正则化</h4>
<div class="outline-text-4" id="text-4-5-6">
<p>
剪枝<br  />
</p>
</div>
</div>

<div id="outline-container-sec-4-5-7" class="outline-4">
<h4 id="sec-4-5-7"><span class="section-number-4">4.5.7</span> Engineering</h4>
<div class="outline-text-4" id="text-4-5-7">
<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">from</span> sklearn.tree <span style="color: #fff59d;">import</span> DecisionTreeClassifier
<span style="color: #fff59d;">from</span> sklearn.tree <span style="color: #fff59d;">import</span> DecisionTreeRegressor
</pre>
</div>
</div>
</div>


<div id="outline-container-sec-4-5-8" class="outline-4">
<h4 id="sec-4-5-8"><span class="section-number-4">4.5.8</span> 优缺点</h4>
<div class="outline-text-4" id="text-4-5-8">
<p>
优点：<br  />
</p>
<ol class="org-ol">
<li>易于理解<br  />
</li>
<li>能够同时处理数据型和常规型属性。<br  />
</li>
<li>效率高。决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。<br  />
</li>
<li>速度快。计算量相对较小, 且容易转化成分类规则。<br  />
</li>
<li>适合高维数据<br  />
</li>
</ol>

<p>
缺点：<br  />
</p>
<ol class="org-ol">
<li>当类别太多时，错误可能就会增加的比较快。<br  />
</li>
<li>在处理特征关联性比较强的数据时表现得不是太好<br  />
</li>
<li>决策树的结果可能是不稳定的，因为在数据中一个很小的变化可能导致生成一个完全不同的树，这个问题可以通过使用集成决策树来解决<br  />
</li>
<li>树的深度过深时，容易造成过拟合<br  />
</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-sec-4-6" class="outline-3">
<h3 id="sec-4-6"><span class="section-number-3">4.6</span> Random Forest</h3>
<div class="outline-text-3" id="text-4-6">
</div><div id="outline-container-sec-4-6-1" class="outline-4">
<h4 id="sec-4-6-1"><span class="section-number-4">4.6.1</span> 特点</h4>
<div class="outline-text-4" id="text-4-6-1">
<ol class="org-ol">
<li>较高的准确率<br  />
</li>
<li>能有效运行在大数据集上<br  />
</li>
<li>能处理高纬特征输入<br  />
</li>
<li>能评估各个特征在分类问题上的重要性<br  />
</li>
</ol>
</div>
</div>

<div id="outline-container-sec-4-6-2" class="outline-4">
<h4 id="sec-4-6-2"><span class="section-number-4">4.6.2</span> 构建</h4>
<div class="outline-text-4" id="text-4-6-2">
<ol class="org-ol">
<li>原始数据集为D，随机有放回的抽取k个子数据集，生成k棵分类树。<br  />
</li>
<li>每个样本特征纬度为M，从M个特征中选取m个子特征集(m&lt;&lt;M)，每次树进行分裂时，从这m个特征中选取最优的。<br  />
</li>
<li>每棵树最大程度生长。<br  />
</li>
<li>综合。如果是离散问题，通过投票选择最优解。如果是连续问题，去期望值。<br  />
</li>
</ol>
</div>
</div>



<div id="outline-container-sec-4-6-3" class="outline-4">
<h4 id="sec-4-6-3"><span class="section-number-4">4.6.3</span> Engineering</h4>
<div class="outline-text-4" id="text-4-6-3">
<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">from</span> sklearn.ensemble <span style="color: #fff59d;">import</span> RandomForestClassifier
<span style="color: #fff59d;">from</span> sklearn.ensemble <span style="color: #fff59d;">import</span> RandomForestRegressor
</pre>
</div>
</div>
</div>
</div>



<div id="outline-container-sec-4-7" class="outline-3">
<h3 id="sec-4-7"><span class="section-number-3">4.7</span> Boost</h3>
<div class="outline-text-3" id="text-4-7">
<p>
boost本义为 提高，促进，改善。<br  />
boost算法就是在原有算法的基础上，进行调整，改善。<br  />
</p>
</div>

<div id="outline-container-sec-4-7-1" class="outline-4">
<h4 id="sec-4-7-1"><span class="section-number-4">4.7.1</span> <span class="done DONE">DONE</span> adaboost</h4>
<div class="outline-text-4" id="text-4-7-1">
<p>
ada: adapt<br  />
</p>

<p>
kernel: weight adapt<br  />
核心思想：权值调整<br  />
</p>
</div>

<ol class="org-ol"><li><a id="sec-4-7-1-1" name="sec-4-7-1-1"></a>算法原理<br  /><div class="outline-text-5" id="text-4-7-1-1">
<p>
假设样本数据集为D，有n个样本，每次训练（就是迭代一次）产生分类器H，共迭代了t次，<br  />
产生了\(H_1 ,H_2 , \cdots , H_t\) 个分类器，第一次迭代时，每个样本的权重都相同，为\(\frac{1}{n}\) 。<br  />
x表示样本特性，y表示类别。<br  />
</p>
</div>

<ol class="org-ol"><li><a id="sec-4-7-1-1-1" name="sec-4-7-1-1-1"></a>第一次训练<br  /><div class="outline-text-6" id="text-4-7-1-1-1">
<p>
以\(W_{1i}\) 表示第一次训练，第i个样本的权重。<br  />
$$
D_1 = (W_{11} , W_{12} , \cdots , W_{1n})
$$<br  />
</p>

<p>
训练完毕，得到分类器\(H_1\) ，该分类器的将样本x分错的概率为：<br  />
</p>
\begin{equation}
\varepsilon_1 = \frac{\sum\limits_{i=1}^{n} (H_1(x_i) \ne y_i)}{n}
\end{equation}

<p>
该分类器的权重\(\alpha_1\) 为：<br  />
$$\alpha_1 = \frac{1}{2} ln(\frac{1 - \varepsilon_1}{\varepsilon_1})$$<br  />
</p>
</div>
</li>

<li><a id="sec-4-7-1-1-2" name="sec-4-7-1-1-2"></a>第二次训练<br  /><div class="outline-text-6" id="text-4-7-1-1-2">
<p>
$$
D_2 = (W_{21} , W_{22} , \cdots , W_{2n})
$$<br  />
</p>

\begin{equation}
W_{2i} =  \left \{ 
\begin{matrix}
W_{1i} , \ \ \ if \ y_i = H_1(x_i) \\
W_{1i} \varepsilon_1 , \ \ \ if \ y_i \ne H_1(x_i)
\end{matrix}
\right.
\end{equation}
</div>
</li>

<li><a id="sec-4-7-1-1-3" name="sec-4-7-1-1-3"></a>第t次训练<br  /><div class="outline-text-6" id="text-4-7-1-1-3">
<p>
$$
D_t = (W_{t1} , W_{t2} , \cdots , W_{tn})
$$<br  />
</p>

\begin{equation}
W_{ti} =  \left \{ 
\begin{matrix}
W_{ti} , \ \ \ if \ y_i = H_{t-1}(x_i) \\
W_{ti} \varepsilon_{t-1} , \ \ \ if \ y_i \ne H_{t-1}(x_i)
\end{matrix}
\right.
\end{equation}
</div>
</li>

<li><a id="sec-4-7-1-1-4" name="sec-4-7-1-1-4"></a>综合所有分类器<br  /><div class="outline-text-6" id="text-4-7-1-1-4">
<p>
$$
H(x) = sign \left ( \sum\limits_{i=1}^t \alpha_t H_i(x) \right )
$$<br  />
</p>
</div>
</li></ol>
</li>

<li><a id="sec-4-7-1-2" name="sec-4-7-1-2"></a>Engineering<br  /><div class="outline-text-5" id="text-4-7-1-2">
<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">from</span> sklearn.ensemble <span style="color: #fff59d;">import</span> AdaBoostClassifier
<span style="color: #fff59d;">from</span> sklearn.ensemble <span style="color: #fff59d;">import</span> AdaBoostRegressor
</pre>
</div>
</div>
</li></ol>
</div>




<div id="outline-container-sec-4-7-2" class="outline-4">
<h4 id="sec-4-7-2"><span class="section-number-4">4.7.2</span> <span class="todo TODO">TODO</span> gradient boost</h4>
<div class="outline-text-4" id="text-4-7-2">
</div><ol class="org-ol"><li><a id="sec-4-7-2-1" name="sec-4-7-2-1"></a>Engineering<br  /><div class="outline-text-5" id="text-4-7-2-1">
<div class="org-src-container">

<pre class="src src-python"><span style="color: #fff59d;">from</span> sklearn.ensemble <span style="color: #fff59d;">import</span> GradientBoostingClassifier
<span style="color: #fff59d;">from</span> sklearn.ensemble <span style="color: #fff59d;">import</span> GradientBoostingRegressor
</pre>
</div>
</div>
</li></ol>
</div>
</div>





<div id="outline-container-sec-4-8" class="outline-3">
<h3 id="sec-4-8"><span class="section-number-3">4.8</span> Linear Regression</h3>
<div class="outline-text-3" id="text-4-8">
<p>
本义： 回归，退化，回到从前。<br  />
即，根据当前样本，找简化的规律。<br  />
</p>

<p>
regression的规律为线的形状。<br  />
这样的规律有很多，哪个才是好的呢？ 这就是评判标准，使评判标准最好的就是最适合的规律。<br  />
在ai中这个评判标准常常指loss function.<br  />
</p>
</div>

<div id="outline-container-sec-4-8-1" class="outline-4">
<h4 id="sec-4-8-1"><span class="section-number-4">4.8.1</span> 目标函数</h4>
<div class="outline-text-4" id="text-4-8-1">
<p>
目标函数就是样本中隐含的规律，用该直线来表示样本的规律。<br  />
$$
y = h_{\theta}(x) = \theta_0 + \theta_{1}x_1 + \cdots + \theta_{n}x_n
$$<br  />
将\(\theta_0\) 乘以\(x_0\) ，其中\(x_0 = 1\) 。<br  />
</p>

<p>
简化为：<br  />
$$
y = h(\theta^{T}x) = \sum_{i=0}^{n}\theta_{i}x_i = \theta^{T}x
$$<br  />
</p>

<p>
其中，\(\theta^T\) 为\((\theta_{0}, \cdots , \theta_{n})\) , 即纬度为n+1的向量, \(x\) 为\(\begin{bmatrix} x_0 \\ \vdots \\ x_n \end{bmatrix}\) 为n+1纬列向量。<br  />
\(x\) 为样本，\(\theta\) 确定目标直线。<br  />
</p>
</div>
</div>



<div id="outline-container-sec-4-8-2" class="outline-4">
<h4 id="sec-4-8-2"><span class="section-number-4">4.8.2</span> 损失函数</h4>
<div class="outline-text-4" id="text-4-8-2">
<p>
理论上应该求解目标函数的，但是\(\theta\) 的取值范围为负无穷到正无穷，计算机无法求解。<br  />
于是通过问题的转化，转化为求损失函数的极小值的问题。<br  />
</p>

<p>
损失函数就是用来评判拟合直线的质量，即用该直线后，会和现实真实产生多大的误差。<br  />
</p>

<p>
loss functions:<br  />
$$
J(\theta)=\frac{1}{2m}\sum^{m}_{i=1}(h(\theta^{T}x_i)-y_i)^2
$$<br  />
其中m为样本的个数，参数2是为了梯度下降法求解时求导的简化。<br  />
</p>
</div>
</div>

<div id="outline-container-sec-4-8-3" class="outline-4">
<h4 id="sec-4-8-3"><span class="section-number-4">4.8.3</span> 梯度下降法求解损失函数</h4>
<div class="outline-text-4" id="text-4-8-3">
<p>
对损失函数求偏导得到：<br  />
$$
\frac{\partial J}{\partial\theta} = \frac{1}{m}\sum_{i=1}^{m}(h(\theta^{T}x_i) - y_i)x_i
$$<br  />
</p>

<p>
计算机梯度下降法求\(\theta\) 的极值方法为：<br  />
repeat:<br  />
$$
\theta_j = \theta_j - \alpha(\frac{1}{m}\sum^{m}_{i=1}(h(\theta^{T}x_i) - y_i)x_i
$$<br  />
其中 \(\alpha\) 为学习率。<br  />
</p>

<p>
上述方法也称为Batch Gradient Descent。<br  />
</p>

<p>
即，为多元函数的时候，每个元都独立在自己的领域内梯度下降。<br  />
</p>

<p>
上式是理论上的梯度下降法法，每次都使用所有的样本点用来计算。<br  />
现实生活中，为了提高计算的效率，使用随机梯度下降法求解，<br  />
即，每次迭代都随机选取一个点来进行计算，而不是所有样本点，<br  />
在牺牲一定准确度的前提下来获得计算速度的提升。<br  />
</p>

<p>
stochastic gradient descent:<br  />
$$\theta_j = \theta_j - \alpha((h(\theta^{T}x_i) - y_i)x_i$$<br  />
和bgd相比，缺少了累加项。<br  />
</p>
</div>
</div>


<div id="outline-container-sec-4-8-4" class="outline-4">
<h4 id="sec-4-8-4"><span class="section-number-4">4.8.4</span> batch or not</h4>
<div class="outline-text-4" id="text-4-8-4">
<p>
<img src="pics/batch_or_not.png" alt="batch_or_not.png" /><br  />
batch: 计算的更准。<br  />
not batch: 计算的更快。<br  />
折中的为： mini batch<br  />
精度和速度一般难以兼顾。<br  />
</p>


<p>
理论数学不同于工程数学，最完美的实现不是最优的实现，<br  />
花费太多的资源来换取微小的准确率的提升，在工程中是不可取的。<br  />
如果资源无限，也就没有了bgd的所谓的优化。<br  />
正如lisp中所言，无限的资源换取无限的逻辑简化(最高等级的抽象)。<br  />
</p>
</div>
</div>

<div id="outline-container-sec-4-8-5" class="outline-4">
<h4 id="sec-4-8-5"><span class="section-number-4">4.8.5</span> simple code</h4>
<div class="outline-text-4" id="text-4-8-5">
<div class="org-src-container">

<pre class="src src-python"></pre>
</div>
</div>
</div>
</div>

<div id="outline-container-sec-4-9" class="outline-3">
<h3 id="sec-4-9"><span class="section-number-3">4.9</span> 逻辑回归  (Logistic Regression)</h3>
<div class="outline-text-3" id="text-4-9">
<p>
天然解决二分类问题的。 (0,1)<br  />
</p>
</div>
<div id="outline-container-sec-4-9-1" class="outline-4">
<h4 id="sec-4-9-1"><span class="section-number-4">4.9.1</span> 目标函数</h4>
<div class="outline-text-4" id="text-4-9-1">
<p>
目标函数：<br  />
$$
\theta_{0}x_0 + \theta_{1}x_1 + \cdots + \theta_{n}x_n = \sum_{x=1}^{n}\theta_{i}x_i = \theta^{T}x 
$$<br  />
</p>

<p>
sigmoid函数，（取值范围为[0,1]），单调，所以用它将负无穷到正无穷映射到[0,1]区间：<br  />
$$
g(x)=\frac{1}{1 + e^x}
$$<br  />
</p>

<p>
组合后的目标函数为：<br  />
$$
h_{\theta}(x) = \frac{1}{1 + e^{-\theta^{T}x}}
$$<br  />
</p>
</div>
</div>

<div id="outline-container-sec-4-9-2" class="outline-4">
<h4 id="sec-4-9-2"><span class="section-number-4">4.9.2</span> 损失函数</h4>
<div class="outline-text-4" id="text-4-9-2">
<p>
以概率的角度理解:<br  />
$$
P(y=1|x,\theta) = h_{\theta}(x)
$$<br  />
</p>

<p>
$$
P(y=0|x,\theta) = 1 - h_{\theta}(x)
$$<br  />
</p>


<p>
设计损失函数：<br  />
$$
P(y|x,\theta) =  (h_{\theta}(x))^{y}(1 - h_{\theta}(x))^{1-y}
$$<br  />
i.e.<br  />
$$
L(\theta) = \prod_{i=1}^{m}P(y|x,\theta) = \prod_{i=1}^{m}(h_{\theta}(x))^{y}(1 - h_{\theta}(x))^{1-y}
$$<br  />
注意y次方和1-y次方，这样设计函数，无论y取0或1，都满足概率公式。<br  />
</p>

<p>
为了简化计算，对两边取对数：<br  />
$$
l(\theta) = lnL(\theta) = \sum_{i=1}^{m}(y_{i}lnh_{\theta}(x_{i}) + (1-y_{i})ln(1-h_{\theta}(x_{i})))
$$<br  />
</p>

<p>
为了求导方便，也为了消除样本个数的影响：<br  />
$$
J(\theta) = -\frac{1}{m}l(\theta)
$$<br  />
</p>

<p>
为什么损失函数越小越好？<br  />
损失函数为两个概率的乘积，而两个概率和为1。<br  />
我们希望的是，y为1时，h的概率接近1，1-h的概率接近0；<br  />
y为0时，h的概率接近0，而1-h的概率接近1，在这两种情况下，损失函数都是接近0的。<br  />
损失函数越大，表示判断正确的概率越低。<br  />
</p>
</div>
</div>


<div id="outline-container-sec-4-9-3" class="outline-4">
<h4 id="sec-4-9-3"><span class="section-number-4">4.9.3</span> 梯度下降求最优参数</h4>
<div class="outline-text-4" id="text-4-9-3">
<p>
对\(\theta\) 中的\(\theta_j\) 求偏导（未推导,思想是一样的）：<br  />
$$
\frac{\partial}{\partial \theta_j}J(\theta) = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x_i) - y_i)x^{j}_i
$$<br  />
</p>

<p>
进行梯度下降迭代求解：<br  />
$$
\theta_j = \theta_j -  \frac{\partial}{\partial \theta_j}J(\theta) = \theta_j - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x_i) - y_i)x^{j}_i
$$<br  />
其中\(\alpha\) 为学习率。<br  />
</p>
</div>
</div>



<div id="outline-container-sec-4-9-4" class="outline-4">
<h4 id="sec-4-9-4"><span class="section-number-4">4.9.4</span> 逻辑回归多分类的实现：</h4>
<div class="outline-text-4" id="text-4-9-4">
<p>
0 -&gt; (yes, no)<br  />
1 -&gt; (yes, no)<br  />
2 -&gt; (yes, no)<br  />
选择概率最大的作为该分类。<br  />
</p>

<p>
即，如果有三类，以此判断出属于第一类的概率和不属于第一类的概率，<br  />
属于第二类和不属于第二类的概率，<br  />
属于第三类和不属于第三类的概率，<br  />
从中选择最大的概率作为该类的分类。<br  />
</p>
</div>
</div>

<div id="outline-container-sec-4-9-5" class="outline-4">
<h4 id="sec-4-9-5"><span class="section-number-4">4.9.5</span> SVM vs LR</h4>
<div class="outline-text-4" id="text-4-9-5">
<p>
svm是从几何角度构建目标函数和损失函数，计算复杂，适合小样本量，准确度。<br  />
lr是从概率的角度构建目标函数和损失函数，计算简单，适合大样本，准确度不如svm。<br  />
</p>
</div>
</div>
</div>



<div id="outline-container-sec-4-10" class="outline-3">
<h3 id="sec-4-10"><span class="section-number-3">4.10</span> Softmax Regression</h3>
<div class="outline-text-3" id="text-4-10">
<p>
解决多分类问题。<br  />
</p>
</div>

<div id="outline-container-sec-4-10-1" class="outline-4">
<h4 id="sec-4-10-1"><span class="section-number-4">4.10.1</span> 目标函数</h4>
<div class="outline-text-4" id="text-4-10-1">
<p>
假设函数对于每一个样本估计其所属的类别的概率为$p(y=j|x)<br  />
</p>

<p>
则目标函数为：<br  />
</p>
\begin{equation}
h_{\theta}(x^{(i)}) =
\begin{bmatrix}
p(y^{(i)} = 1 | x^{(i)}; \theta) \\
p(y^{(i)} = 2 | x^{(i)}; \theta) \\
\vdots \\
p(y^{(i)} = k | x^{(i)}; \theta)
\end{bmatrix}

= \frac{1}{\sum_{j=1}^{k}e^{\theta_{j}^{T}}x^{(i)}} \ 
\begin{bmatrix}
e^{\theta_{1}^{T}x^{(i)}} \\
e^{\theta_{2}^{T}x^{(i)}} \\
\vdots  \\
e^{\theta_{k}^{T}x^{(i)}} 
\end{bmatrix}
\end{equation}

<p>
其中，y总共有k个类别，x为样本。<br  />
矩阵前面的系数是为了归一化。<br  />
</p>

<p>
则样本x所属的类别为矩阵中概率最大的那个类别。<br  />
</p>
</div>
</div>


<div id="outline-container-sec-4-10-2" class="outline-4">
<h4 id="sec-4-10-2"><span class="section-number-4">4.10.2</span> <span class="done DONE">DONE</span> 损失函数</h4>
<div class="outline-text-4" id="text-4-10-2">
<p>
引入辅助函数：<br  />
</p>
\begin{equation}
I\{expression\} = 
\begin{cases}
0 & if expression = false \\
1 & if expression = true
\end{cases}
\end{equation}

<p>
损失函数为交叉熵：<br  />
</p>
\begin{equation}
J(\theta) = -\frac{1}{m} [\sum_{i=1}^{m}\sum_{j=1}^{k} I\{y^{(i)} = j\} ln\frac{e^{\theta_{j}^{T}x^{(i)}}}{\sum_{i=1}^{k}e^{\theta_{j}^{T}x^{(i)}}}]
\end{equation}
<p>
m为样品本数量，k为类别数，ln是为了将概率的累乘变为概率的累加。<br  />
负号是为了从最大变为最小，还是求解极小值。<br  />
</p>

<p>
上式表示，对所有样本所有类别的概率进行累乘。<br  />
因为辅助函数的存在，当样本的类别为j时，即使对所有类别进行了累加，其实仅仅保留了j类别的项，其余非j类别的项都为0.<br  />
</p>
</div>
</div>

<div id="outline-container-sec-4-10-3" class="outline-4">
<h4 id="sec-4-10-3"><span class="section-number-4">4.10.3</span> <span class="todo TODO">TODO</span> 梯度下降法求解</h4>
<div class="outline-text-4" id="text-4-10-3">
\begin{equation}
\theta_j = \theta_j - \alpha \nabla_{\theta_j} J(\theta)
\end{equation}
</div>
</div>
</div>

<div id="outline-container-sec-4-11" class="outline-3">
<h3 id="sec-4-11"><span class="section-number-3">4.11</span> EM</h3>
<div class="outline-text-3" id="text-4-11">
</div><div id="outline-container-sec-4-11-1" class="outline-4">
<h4 id="sec-4-11-1"><span class="section-number-4">4.11.1</span> 聚类</h4>
<div class="outline-text-4" id="text-4-11-1">
<p>
将物理或抽象对象的集合划分成相似的对象类的过程称为聚类。<br  />
</p>
</div>
</div>

<div id="outline-container-sec-4-11-2" class="outline-4">
<h4 id="sec-4-11-2"><span class="section-number-4">4.11.2</span> 实验</h4>
<div class="outline-text-4" id="text-4-11-2">
<p>
实验：<br  />
有三枚硬币，a,b,c,<br  />
先抛a，如果a正面，则抛b，<br  />
如果a反面，则抛c，<br  />
将b或c的结果记录，正面为1，反面为0，<br  />
</p>

<p>
现在有结果 [0,1,0,1]<br  />
估计abc是多少。<br  />
</p>
</div>
</div>

<div id="outline-container-sec-4-11-3" class="outline-4">
<h4 id="sec-4-11-3"><span class="section-number-4">4.11.3</span> 分析</h4>
<div class="outline-text-4" id="text-4-11-3">
<p>
其中a为隐藏变量，问题是带有隐藏变量的MLE。<br  />
（它代表了所有的无监督的聚类算法）<br  />
</p>
</div>
</div>


<div id="outline-container-sec-4-11-4" class="outline-4">
<h4 id="sec-4-11-4"><span class="section-number-4">4.11.4</span> E - 隐藏变量的分布</h4>
<div class="outline-text-4" id="text-4-11-4">
<p>
假设abc已知，结果0或1为\(y_i\) .<br  />
在abc和\(y_i\) 发生的前提下，抛硬币b的概率为：<br  />
</p>
\begin{equation}
\mu_i = P(B|y_{i}) = \frac{P(B)P(y_i | B)}{P(y_i)} =  \frac{P(B)P(y_i | B)}{P(y_i | B) + P(y_i | \lnot B)}
\end{equation}

<p>
其中：<br  />
</p>
\begin{matrix}
P(B) = a \\
P(y_i|B) = b^{y_i}(1-b)^{1-y_i} \\
P(y_i |B) = ab^{y_i}(1-b)^{1-y_i} \\
P(y_i | \lnot B) = (1-a)c^{y_i}(1-c)^{1-y_i} 
\end{matrix}

<p>
所以：<br  />
</p>
\begin{equation}
\mu_i = \frac{ab^{y_i}(1-b)^{1-y_i}}{ab^{y_i}(1-b)^{1-y_i}  + (1-a)c^{y_i}(1-c)^{1-y_i}}
\end{equation}
</div>
</div>

<div id="outline-container-sec-4-11-5" class="outline-4">
<h4 id="sec-4-11-5"><span class="section-number-4">4.11.5</span> M - 参数最大化</h4>
<div class="outline-text-4" id="text-4-11-5">
<p>
通过求导，求解。<br  />
</p>

\begin{matrix}
a = \frac{1}{n}\sum_i \mu_i\\
b = \frac{\sum_i \mu_i y_i}{\sum_i{\mu_i}}\\
c = \frac{\sum_i (1-\mu_i) y_i}{\sum_i{(1-\mu_i)}}\\
\end{matrix}
</div>
</div>


<div id="outline-container-sec-4-11-6" class="outline-4">
<h4 id="sec-4-11-6"><span class="section-number-4">4.11.6</span> 例子</h4>
<div class="outline-text-4" id="text-4-11-6">
<p>
K-means算法<br  />
</p>

<p>
分布：<br  />
<img src="pics/k-means1.png" alt="k-means1.png" /><br  />
</p>

<p>
隐藏的变量为：样本点是属于哪一类的。<br  />
</p>
</div>

<ol class="org-ol"><li><a id="sec-4-11-6-1" name="sec-4-11-6-1"></a>步骤<br  /><div class="outline-text-5" id="text-4-11-6-1">
<ol class="org-ol">
<li>随机选择三个中心，图中的三个三角。<br  />
</li>
<li>在三个中心已知的情况下，估计隐藏变量的分布。(距离那个中心点近就是属于哪一类的) (E step)。<br  />
</li>
<li>估计隐藏变量的中心，在这个类中，参数最大化，使聚类中心选择最合理的位置 (M step)。<br  />
</li>
<li>重复2，3步，直到误差满足条件，终止迭代。<br  />
</li>
</ol>
</div>
</li>

<li><a id="sec-4-11-6-2" name="sec-4-11-6-2"></a>示意图<br  /><div class="outline-text-5" id="text-4-11-6-2">
<p>
<img src="pics/k-means1.png" alt="k-means1.png" /><br  />
<img src="pics/k-means2.png" alt="k-means2.png" /><br  />
<img src="pics/k-means3.png" alt="k-means3.png" /><br  />
<img src="pics/k-means4.png" alt="k-means4.png" /><br  />
<img src="pics/k-means5.png" alt="k-means5.png" /><br  />
<img src="pics/k-means6.png" alt="k-means6.png" /><br  />
<img src="pics/k-means8.png" alt="k-means8.png" /><br  />
<img src="pics/k-means9.png" alt="k-means9.png" /><br  />
</p>
</div>
</li></ol>
</div>

<div id="outline-container-sec-4-11-7" class="outline-4">
<h4 id="sec-4-11-7"><span class="section-number-4">4.11.7</span> <span class="done DONE">DONE</span> code</h4>
<div class="outline-text-4" id="text-4-11-7">
<div class="org-src-container">

<pre class="src src-python"><span style="color: #ffe4b5;">"""</span>
<span style="color: #ffe4b5;">EM algorithm</span>
<span style="color: #ffe4b5;">"""</span>
<span style="color: #fff59d;">import</span> numpy <span style="color: #fff59d;">as</span> np
<span style="color: #fff59d;">import</span> matplotlib.pyplot <span style="color: #fff59d;">as</span> plt

<span style="color: #b0bec5;"># </span><span style="color: #b0bec5;">1000 sample</span>
<span style="color: #b0bec5;"># </span><span style="color: #b0bec5;">three coin: A B C</span>
<span style="color: #ffcc80;">a_true</span> = 0.3
<span style="color: #ffcc80;">b_true</span> = 0.2
<span style="color: #ffcc80;">c_true</span> = 0.8


<span style="color: #fff59d;">def</span> <span style="color: #84ffff;">make_sample</span>(num, p1, p2, p3):
<span style="background-color: #37474f;"> </span>   <span style="color: #ffcc80;">data</span> = []
<span style="background-color: #37474f;"> </span>   <span style="color: #fff59d;">for</span> itr <span style="color: #fff59d;">in</span> <span style="color: #ff8A65;">range</span>(num):
<span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="color: #b0bec5;"># </span><span style="color: #b0bec5;">coin A</span>
<span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="color: #ffcc80;">A</span> = np.random.random()
<span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="color: #fff59d;">if</span> A &lt; p1:
<span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="color: #b0bec5;"># </span><span style="color: #b0bec5;">coin B</span>
<span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="color: #ffcc80;">B</span> = np.random.random()
<span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="color: #fff59d;">if</span> B &lt; p2:
<span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   data.append(1)
<span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="color: #fff59d;">else</span>:
<span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   data.append(0)
<span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="color: #fff59d;">else</span>:
<span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="color: #b0bec5;"># </span><span style="color: #b0bec5;">coin C</span>
<span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="color: #ffcc80;">C</span> = np.random.random()
<span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="color: #fff59d;">if</span> C &lt; p3:
<span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   data.append(1)
<span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="color: #fff59d;">else</span>:
<span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   <span style="background-color: #37474f;"> </span>   data.append(0)
<span style="background-color: #37474f;"> </span>   <span style="color: #fff59d;">return</span> np.array(data)


<span style="color: #ffcc80;">data</span> = make_sample(1000, a_true, b_true, c_true)
plt.subplot(211)
plt.hist(data)

<span style="color: #b0bec5;"># </span><span style="color: #b0bec5;">assumption</span>
<span style="color: #ffcc80;">a</span>, <span style="color: #ffcc80;">b</span>, <span style="color: #ffcc80;">c</span> = .4, .5, .7
<span style="color: #fff59d;">for</span> step <span style="color: #fff59d;">in</span> <span style="color: #ff8A65;">range</span>(20):
<span style="background-color: #37474f;"> </span>   <span style="color: #b0bec5;"># </span><span style="color: #b0bec5;">E-step</span>
<span style="background-color: #37474f;"> </span>   <span style="color: #ffcc80;">m1</span> = a * b ** data * (1 - b) ** (1 - data)
<span style="background-color: #37474f;"> </span>   <span style="color: #ffcc80;">m2</span> = (1 - a) * c ** data * (1 - c) ** (1 - data)
<span style="background-color: #37474f;"> </span>   <span style="color: #ffcc80;">mu</span> = m1 / (m1 + m2)
<span style="background-color: #37474f;"> </span>   <span style="color: #b0bec5;"># </span><span style="color: #b0bec5;">M-step</span>
<span style="background-color: #37474f;"> </span>   <span style="color: #ffcc80;">a</span> = np.mean(mu)
<span style="background-color: #37474f;"> </span>   <span style="color: #ffcc80;">b</span> = np.<span style="color: #ff8A65;">sum</span>(mu * data) / np.<span style="color: #ff8A65;">sum</span>(mu)
<span style="background-color: #37474f;"> </span>   <span style="color: #ffcc80;">c</span> = np.<span style="color: #ff8A65;">sum</span>((1 - mu) * data) / np.<span style="color: #ff8A65;">sum</span>(1 - mu)
<span style="background-color: #37474f;"> </span>   <span style="color: #fff59d;">print</span>(<span style="color: #9ccc65;">'step: {step}, true: ({a_true:.2f}, {b_true:.2f}, {c_true:.2f}), pred: ({a:.2f}, {b:.2f}, {c:.2f})'</span>.<span style="color: #ff8A65;">format</span>(**<span style="color: #ff8A65;">locals</span>()))

<span style="color: #b0bec5;"># </span><span style="color: #b0bec5;">predict</span>
<span style="color: #ffcc80;">data</span> = make_sample(1000, a, b, c)
plt.subplot(212)
plt.hist(data)
plt.show()
</pre>
</div>

<p>
输出结果为：<br  />
</p>
<pre class="example">
step: 0, true: (0.30, 0.20, 0.80), pred: (0.40, 0.51, 0.71)
step: 1, true: (0.30, 0.20, 0.80), pred: (0.40, 0.51, 0.71)
step: 2, true: (0.30, 0.20, 0.80), pred: (0.40, 0.51, 0.71)
step: 3, true: (0.30, 0.20, 0.80), pred: (0.40, 0.51, 0.71)
step: 4, true: (0.30, 0.20, 0.80), pred: (0.40, 0.51, 0.71)
step: 5, true: (0.30, 0.20, 0.80), pred: (0.40, 0.51, 0.71)
step: 6, true: (0.30, 0.20, 0.80), pred: (0.40, 0.51, 0.71)
step: 7, true: (0.30, 0.20, 0.80), pred: (0.40, 0.51, 0.71)
step: 8, true: (0.30, 0.20, 0.80), pred: (0.40, 0.51, 0.71)
step: 9, true: (0.30, 0.20, 0.80), pred: (0.40, 0.51, 0.71)
step: 10, true: (0.30, 0.20, 0.80), pred: (0.40, 0.51, 0.71)
step: 11, true: (0.30, 0.20, 0.80), pred: (0.40, 0.51, 0.71)
step: 12, true: (0.30, 0.20, 0.80), pred: (0.40, 0.51, 0.71)
step: 13, true: (0.30, 0.20, 0.80), pred: (0.40, 0.51, 0.71)
step: 14, true: (0.30, 0.20, 0.80), pred: (0.40, 0.51, 0.71)
step: 15, true: (0.30, 0.20, 0.80), pred: (0.40, 0.51, 0.71)
step: 16, true: (0.30, 0.20, 0.80), pred: (0.40, 0.51, 0.71)
step: 17, true: (0.30, 0.20, 0.80), pred: (0.40, 0.51, 0.71)
step: 18, true: (0.30, 0.20, 0.80), pred: (0.40, 0.51, 0.71)
step: 19, true: (0.30, 0.20, 0.80), pred: (0.40, 0.51, 0.71)
</pre>

<p>
绘制图形为：<br  />
<img src="pics/coin_em.png" alt="coin_em.png" /><br  />
</p>

<p>
分析结果可知：<br  />
结果具有多解性，不同的概率值也可取得和实际结果相似的分析结果。<br  />
</p>
</div>
</div>


<div id="outline-container-sec-4-11-8" class="outline-4">
<h4 id="sec-4-11-8"><span class="section-number-4">4.11.8</span> 多解性(how to understand this?)</h4>
<div class="outline-text-4" id="text-4-11-8">
<p>
EM本身具有多解性。<br  />
</p>

<p>
解的稳定性：<br  />
给不同的初始值，得到的结果是确定的。<br  />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-4-12" class="outline-3">
<h3 id="sec-4-12"><span class="section-number-3">4.12</span> LDA</h3>
</div>

<div id="outline-container-sec-4-13" class="outline-3">
<h3 id="sec-4-13"><span class="section-number-3">4.13</span> 协同过滤推荐(collaborative filtering)</h3>
<div class="outline-text-3" id="text-4-13">
</div><div id="outline-container-sec-4-13-1" class="outline-4">
<h4 id="sec-4-13-1"><span class="section-number-4">4.13.1</span> used-based CF</h4>
</div>

<div id="outline-container-sec-4-13-2" class="outline-4">
<h4 id="sec-4-13-2"><span class="section-number-4">4.13.2</span> item-based CF</h4>
</div>

<div id="outline-container-sec-4-13-3" class="outline-4">
<h4 id="sec-4-13-3"><span class="section-number-4">4.13.3</span> model-based CF</h4>
</div>
</div>
</div>

<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5"><span class="section-number-2">5</span> 距离</h2>
<div class="outline-text-2" id="text-5">
<p>
在聚类算法中，比较重要的是距离的度量.<br  />
</p>
</div>
<div id="outline-container-sec-5-1" class="outline-3">
<h3 id="sec-5-1"><span class="section-number-3">5.1</span> Euclidean Distance</h3>
<div class="outline-text-3" id="text-5-1">
<p>
$$
(x_1 - x_2)^2
$$<br  />
<img src="pics/euclidean.png" alt="euclidean.png" /><br  />
</p>
</div>
</div>

<div id="outline-container-sec-5-2" class="outline-3">
<h3 id="sec-5-2"><span class="section-number-3">5.2</span> Cosine Distance</h3>
<div class="outline-text-3" id="text-5-2">
<p>
$$
\frac{x_1 \cdot x_2}{|x_1| |x_2|}
$$<br  />
<img src="pics/cosine.png" alt="cosine.png" /><br  />
</p>
</div>
</div>

<div id="outline-container-sec-5-3" class="outline-3">
<h3 id="sec-5-3"><span class="section-number-3">5.3</span> Manhattan Distance</h3>
<div class="outline-text-3" id="text-5-3">
\begin{equation}
|x_1 - x_2|
\end{equation}
</div>
</div>

<div id="outline-container-sec-5-4" class="outline-3">
<h3 id="sec-5-4"><span class="section-number-3">5.4</span> Mahalanob Distance</h3>
<div class="outline-text-3" id="text-5-4">

<div class="figure">
<p><img src="pics/mahalanob.png" alt="mahalanob.png" /><br  />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-5-5" class="outline-3">
<h3 id="sec-5-5"><span class="section-number-3">5.5</span> PearsonCorrelation</h3>
<div class="outline-text-3" id="text-5-5">
<p>
皮尔逊相关系数具有平移不变性和尺度不变性<br  />
计算出了两个向量（维度）的相关性。<br  />
不过，一般我们在谈论相关系数的时候，<br  />
将 x 与 y 对应位置的两个数值看作一个样本点，<br  />
皮尔逊系数用来表示这些样本点分布的相关性。<br  />
</p>

<p>
$$
\rho_{X,Y} = \frac{cov(X,Y)}{\sigma_X \sigma_Y} = \frac{E[(X - \mu_X)(Y-\mu_Y)]}{\sigma_X \sigma_Y}
$$<br  />
</p>


<div class="figure">
<p><img src="pics/pearson.png" alt="pearson.png" /><br  />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-5-6" class="outline-3">
<h3 id="sec-5-6"><span class="section-number-3">5.6</span> KL散度 - 相对熵</h3>
<div class="outline-text-3" id="text-5-6">
<p>
p,q是两个概率分布。<br  />
</p>

<p>
$$
KL(p||q)=\sum p(i)log\frac{p(i)}{q(i)}
$$<br  />
</p>

<p>
相对熵可以衡量两个随机分布之间的距离，当两个随机分布相同时，它们的相对熵为零，<br  />
当两个随机分布的差别增大时，它们的相对熵也会增大。<br  />
所以相对熵（KL散度）可以用于比较文本的相似度，先统计出词的频率，然后计算KL散度就行了。<br  />
</p>
</div>
</div>

<div id="outline-container-sec-5-7" class="outline-3">
<h3 id="sec-5-7"><span class="section-number-3">5.7</span> 交叉熵</h3>
<div class="outline-text-3" id="text-5-7">
<p>
主要用于度量两个概率分布间的差异性信息。<br  />
</p>

<p>
在信息论中，交叉熵是表示两个概率分布p,q，其中p表示真实分布，q表示非真实分布，<br  />
在相同的一组事件中，其中，用非真实分布q来表示某个事件发生所需要的平均比特数。<br  />
</p>

<p>
假设现在有一个样本集中两个概率分布p,q，其中p为真实分布，q为非真实分布。<br  />
假如，按照真实分布p来衡量识别一个样本所需要的编码长度的期望为：<br  />
$$
H(p)= \sum p(i)\cdot log(\frac{1}{p(i)})
$$<br  />
但是，如果采用错误的分布q来表示来自真实分布p的平均编码长度，则应该是：<br  />
$$
H(p,q)= \sum p(i)\cdot log(\frac{1}{q(i)})
$$ <br  />
此时就将H(p,q)称之为交叉熵。<br  />
</p>
</div>
</div>


<div id="outline-container-sec-5-8" class="outline-3">
<h3 id="sec-5-8"><span class="section-number-3">5.8</span> Hamming Distance</h3>
<div class="outline-text-3" id="text-5-8">
<p>
两个等长字符串s1与s2之间, 将其中一个变为另外一个所需要作的最小替换次数。<br  />
</p>
</div>
</div>
<div id="outline-container-sec-5-9" class="outline-3">
<h3 id="sec-5-9"><span class="section-number-3">5.9</span> Edit Distance</h3>
<div class="outline-text-3" id="text-5-9">
<p>
指两个字串之间，由一个转成另一个所需的最少编辑操作次数。<br  />
许可的编辑操作包括：将一个字符替换成另一个字符，插入一个字符，删除一个字符。<br  />
</p>
</div>
</div>
<div id="outline-container-sec-5-10" class="outline-3">
<h3 id="sec-5-10"><span class="section-number-3">5.10</span> Chebyshev Distance</h3>
<div class="outline-text-3" id="text-5-10">
<p>
max(|x1-x2|,|y1-y2|,&#x2026;)<br  />
</p>


<div class="figure">
<p><img src="pics/chebyshev.png" alt="chebyshev.png" /><br  />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-5-11" class="outline-3">
<h3 id="sec-5-11"><span class="section-number-3">5.11</span> Inner Distance</h3>
<div class="outline-text-3" id="text-5-11">
<p>
两个向量相乘。<br  />
$$
x_1 \cdot x_2
$$<br  />
</p>


<div class="figure">
<p><img src="pics/inner.png" alt="inner.png" /><br  />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-5-12" class="outline-3">
<h3 id="sec-5-12"><span class="section-number-3">5.12</span> Jaccard Distance</h3>
<div class="outline-text-3" id="text-5-12">
<p>
举个例子来说:<br  />
电影基数非常庞大<br  />
用户看过的电影只占其中非常小的一部分<br  />
如果两个用户都没有看过某一部电影（两个都是 0）<br  />
并不能说明两者相似<br  />
反而言之，如果两个用户都看过某一部电影（序列中都是 1）<br  />
则说明用户有很大的相似度。<br  />
在这个例子中，序列中等于 1 所占的权重应该远远大于 0 的权重<br  />
</p>

<p>
$$
\frac{c_{TF} + c_{FT}}
{c_{TT} + c_{FT} + c_{TF}}
$$<br  />
</p>

<p>
where \(c_{ij}\) is the number of occurrences of<br  />
\(\mathtt{u[k]} = i\) and \(\mathtt{v[k]} = j\) for \(k < n\) .<br  />
</p>

<p>
Parameters:<br  />
u : (N,) array_like, bool<br  />
v : (N,) array_like, bool<br  />
</p>
</div>
</div>
</div>



<div id="outline-container-sec-6" class="outline-2">
<h2 id="sec-6"><span class="section-number-2">6</span> 正则化</h2>
<div class="outline-text-2" id="text-6">
<ul class="org-ul">
<li>为了防止过拟合，你和过程中通常倾向于让权值尽可能小，即降低模型复杂度<br  />
</li>
<li>参数值小的模型比较简单，能适应不同的数据集，在一定程度上避免了过拟合现象<br  />
</li>
<li>能减小样本数据对规律的扰动<br  />
</li>
</ul>

<p>
过拟合导致的原因为：有些特征，训练集中有，而训练集中没有。<br  />
</p>
</div>

<div id="outline-container-sec-6-1" class="outline-3">
<h3 id="sec-6-1"><span class="section-number-3">6.1</span> L-P范数</h3>
<div class="outline-text-3" id="text-6-1">
<p>
距离定义是一个宽泛的概念，只要满足非负，自反，三角不等式就可以称为距离。<br  />
范数是一种强化了的距离概念，在定义上多了一条数乘的运算法则。<br  />
</p>

<p>
L-P范数是一组范数，定义如下：<br  />
</p>
\begin{equation}
L_p = ||X||_p = \sqrt[p]{\sum_{i=1}^{n} x_{i}^{p}} \ \ \ \  X = (x_1 , x_2 , \cdots , x_n )
\end{equation}

<p>
范数示意图如下：<br  />
<img src="pics/lp.png" alt="lp.png" /><br  />
动态变化图如下：<br  />
<img src="pics/Lp_space_animation.gif" alt="Lp_space_animation.gif" /><br  />
</p>
</div>
</div>

<div id="outline-container-sec-6-2" class="outline-3">
<h3 id="sec-6-2"><span class="section-number-3">6.2</span> L0</h3>
<div class="outline-text-3" id="text-6-2">
<p>
L0并非一个真正的范数，主要用来衡量向量中非零元素的个数。<br  />
</p>
</div>
</div>

<div id="outline-container-sec-6-3" class="outline-3">
<h3 id="sec-6-3"><span class="section-number-3">6.3</span> L1</h3>
<div class="outline-text-3" id="text-6-3">
\begin{equation}
\lVert X \rVert_1 = \sum_{i=1}^{n} \lvert x_i \rvert
\end{equation}
</div>
</div>

<div id="outline-container-sec-6-4" class="outline-3">
<h3 id="sec-6-4"><span class="section-number-3">6.4</span> L2</h3>
<div class="outline-text-3" id="text-6-4">
\begin{equation}
\lVert X \rVert_2 = \sqrt {\sum_{i=1}^{n} x_{i}^2}
\end{equation}
</div>
</div>

<div id="outline-container-sec-6-5" class="outline-3">
<h3 id="sec-6-5"><span class="section-number-3">6.5</span> 正则化求解</h3>
<div class="outline-text-3" id="text-6-5">
<p>
带约束极值求解问题，都通过拉格朗日变换，转化为无约束问题的求解。<br  />
</p>
</div>
</div>

<div id="outline-container-sec-6-6" class="outline-3">
<h3 id="sec-6-6"><span class="section-number-3">6.6</span> Lasso</h3>
<div class="outline-text-3" id="text-6-6">
<p>
线性回归模型加上L1约束<br  />
</p>
</div>
</div>

<div id="outline-container-sec-6-7" class="outline-3">
<h3 id="sec-6-7"><span class="section-number-3">6.7</span> Ridge</h3>
<div class="outline-text-3" id="text-6-7">
<p>
线性回归模型加上L2约束<br  />
</p>
</div>
</div>
</div>



<div id="outline-container-sec-7" class="outline-2">
<h2 id="sec-7"><span class="section-number-2">7</span> 牛顿法</h2>
<div class="outline-text-2" id="text-7">
<p>
牛顿法比梯度下降法收敛更快（使用了二阶导数）（迭代次数更少）<br  />
代价就是： 计算更复杂<br  />
</p>

<p>
牛顿法要求函数具有二阶连续可导。<br  />
</p>

<p>
泰勒展开：<br  />
$$
f(x) = \frac{f(x_0)}{0!} + \frac{f^{'}(x_0)}{1!}(x-x_0) + \frac{f^{''}(x_0)}{2!}(x-x_0)^2 + \cdots + R_{n}(x)
$$<br  />
</p>

<p>
牛顿法使用的为：<br  />
$$
f(x) \approx \frac{f(x_0)}{0!} + \frac{f^{'}(x_0)}{1!}(x-x_0) + \frac{f^{''}(x_0)}{2!}(x-x_0)^2
$$<br  />
</p>


<p>
函数两边对x求导：<br  />
$$
0 = f^{'}(x_0) + f^{''}(x_0)(x-x_0)
$$<br  />
</p>

<p>
进而求得：<br  />
$$
x = x_0 - \frac{f^{'}(x_0)}{f^{''}(x_0)}
$$<br  />
以上就是迭代方式。<br  />
</p>

<p>
同样的推导方式推广到n纬，导数换为偏导数。<br  />
</p>
</div>
</div>

<div id="outline-container-sec-8" class="outline-2">
<h2 id="sec-8"><span class="section-number-2">8</span> 频率学派和Bayes学派</h2>
<div class="outline-text-2" id="text-8">
<p>
频率学派和Bayes学派最大的不同在于先验概率的不同。<br  />
频率学派认为，造成这个实验的结果是独立的，根据这个结果计算得到这个结果的最优化解。<br  />
Bayes学派认为，有先验概率的存在，实验的结果无法代表规律的本身，这是逼近规律本身，而是在先验规律的基础上对规律进行修正，使规律更符合实验结果。<br  />
</p>

<p>
当先验概率为均匀分布时，相当于最大熵分布，最低信息含量，<br  />
这个时候，Bayes学派退化为频率学派。<br  />
</p>
</div>

<div id="outline-container-sec-8-1" class="outline-3">
<h3 id="sec-8-1"><span class="section-number-3">8.1</span> 最大似然估计</h3>
<div class="outline-text-3" id="text-8-1">
<p>
maximum likelihood estimation<br  />
</p>

<p>
MLE是频率学派求解概率的假设依据。<br  />
</p>

<p>
即，在结果发生的前提下，假设概率为\(P(\theta)\) ，<br  />
那么既然这个结果发生了，就认为\(P(\theta)\) 应该使这个结果发生的概率最大。<br  />
进而求得在这个结果发生的前提下，概率的值。<br  />
</p>
</div>
</div>

<div id="outline-container-sec-8-2" class="outline-3">
<h3 id="sec-8-2"><span class="section-number-3">8.2</span> 最大后验概率</h3>
<div class="outline-text-3" id="text-8-2">
<p>
maximum a posteriori <i>posti 'ri o ri</i><br  />
在加入先验概率的情况下，求解MLE<br  />
</p>
</div>
</div>

<div id="outline-container-sec-8-3" class="outline-3">
<h3 id="sec-8-3"><span class="section-number-3">8.3</span> 规律与实验次数</h3>
<div class="outline-text-3" id="text-8-3">
<p>
我们是通过实验结果去推测隐含的规律，<br  />
当实验次数越多的时候，相当于对规律限制的条件越多，<br  />
那么规律越精确。<br  />
</p>

<p>
如下图所示：<br  />
<img src="pics/n1.png" alt="n1.png" /><br  />
<img src="pics/n2.png" alt="n2.png" /><br  />
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Created: 2019-12-09 Mon 15:17</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
