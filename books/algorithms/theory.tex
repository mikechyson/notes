
\chapter{Theory}

\section{What is algorithm?}

\begin{tcolorbox}
  \begin{equation*}
    input \longrightarrow algorithm \longrightarrow output    
  \end{equation*}
\end{tcolorbox}


An algorithm is a sequence of computational that transform the input into the output, it describe a specific computational procedure for achieving the input/output relationship.


\section{Instance of a problem}

An instance of a problem is the input needed to compute a solution to the problem.

\section{Correct algorithm}

For every input instance, a correct algorithm halts out the corrent output.

\section{Data structure}

Data structure is a way to store and organize data in order to faciliate access and modifications.
No single data structure works well for all purpose.

\section{The core technique}

Learning an algorithm is to learn the technique of algorithm design and analysis.

\section{Algorithm efficiency}

Computers are not infinitely fast and memory is not free, thus the efficiency of a algorithm matters.


\section{Algorithms use information}


There is information in the problem, the general is, the more information you use, the more efficiency the algorithm is.
Data structure is a way to use this information.


\section{Loop invariant}

There can be infinite input cases, so it is hard to say an algorithm is corrent.
Loop invariant is a way to guarantee that the algorithm is correct.


There are 3 elements in a loop invariant:
\begin{description}
\item[initialization] It is true prior to the first iteration of the loop.
\item[maintanance] If it is true before an iteration of the loop, it remains true before the next iteration.
\item[termination] When the loop terminates, the invariant gives us a useful property that helps show that the algorithm is correct.
\end{description}


\section{Analyzing an algorithm}

Analyzing an algorithm is to predict the resources consumed.
The most important resources is time and space.

In general, the time grows with the size of the input,
so it is traditional to describe the running time as the function of the size of its input.

\subsection{Worst-case analysis}

Becuase the behavior of an algorithm may be different for each possible input, we need a means for summarizing that behavior in simple, easily understood formulas.
The reason to analyze worst-case running time is as follows:
\begin{enumerate}
\item It give an upper bound on the running time.
\item Worst case ocurrs fairly often.
\item The ``average case'' is often roughly as bad as the worst case.
\end{enumerate}


\section{Resource model}


Before we can analyze an algorithm, we must have a model of the implementation technology that we will use, including a model for the resources of that technology and their costs. 
However, the focus is algorithm, not the tedious hardware detail.
We shall assume a generic one-processor, random-access machine (RAM) model of computation as our implementation technology and understand
that our algorithms will be implemented as computer programs.
In the RAM model, instructions are executed one after another, with no concurrent operations.



\section{Growth of functions}

Althoght we can sometimes determine the exact running time of an algorithm, the extra procision is not usually worth the effort of computing it.
When we look at input sizes large enought to make only the order of growth of the running time relevant, we are studying the \keyword{asymptotic efficiency} of algorithms.


\subsection{$\Theta$-notation}

\begin{gather*}
  \Theta(g(n)) = \{ f(n): \mathrm{there\ exist\ positive\ constant\ } c_1, c_2\ \mathrm{and} \ n_0 \\
  \mathrm{such\ that\ } 0\le c_1 g(n) \le f(n) \le c_2 g(n) \mathrm{\ for\ all\ } n\ge n_0 \}
\end{gather*}

Because $\Theta(g(n))$ is a set, we could write ``$f(n) \in \Theta(g(n))$'' to indicate that $f(n)$ is a memeber of $\Theta(g(n))$.
However, we will usually write ``$f(n)=\Theta(g(n))$'' to express the same notion.

$\Theta$-notation indicates the function is bounded with a asymptotically tight upper bound and a asymptotically tight lower bound.

\subsection{O-notation}

\begin{gather*}
  O(g(n)) = \{ f(n): \mathrm{there\ exist\ positive\ constant\ } c\ \mathrm{and} \ n_0 \\
  \mathrm{such\ that\ } 0\le f(n) \le c g(n) \mathrm{\ for\ all\ } n\ge n_0 \}
\end{gather*}

$O$-notation indicates the function is bounded with a asymptotically tight upper bound.

\subsection{$\Omega$-notation}


\begin{gather*}
  \Omega(g(n)) = \{ f(n): \mathrm{there\ exist\ positive\ constant\ } c\ \mathrm{and} \ n_0 \\
  \mathrm{such\ that\ } 0 \le c g(n) \le f(n) \mathrm{\ for\ all\ } n\ge n_0 \}
\end{gather*}


$\Omega$-notation indicates the function is bounded with a asymptotically tight lower bound.



\subsection{Theorem}

For any two functions $f(n)$ and $g(n)$,
we have $f(n) = \Theta(g(n))$ if and only if
$f(n) = O(g(n))$ and $f(n) = \Omega(g(n))$.


\subsection{o-notation}

\begin{gather*}
  O(g(n)) = \{ f(n): \mathrm{for\ any\ positive\ constant\ } c\ \mathrm{there\ exist\ a\ constant\ } n_0 > 0 \\
  \mathrm{such\ that\ } 0\le f(n) < c g(n) \mathrm{\ for\ all\ } n\ge n_0 \}
\end{gather*}

or
\begin{equation*}
  \lim_{n\rightarrow\infty}\frac{f(n)}{g(n)} = 0
\end{equation*}

$o$-notation indicates the function is bounded with a not asymptotically tight upper bound.


\subsection{$\omega$-notation}


\begin{gather*}
  O(g(n)) = \{ f(n): \mathrm{for\ any\ positive\ constant\ } c\ \mathrm{there\ exist\ a\ constant\ } n_0 > 0 \\
  \mathrm{such\ that\ } 0 \le \le c g(n) < f(n) \mathrm{\ for\ all\ } n\ge n_0 \}
\end{gather*}

or
\begin{equation*}
  \lim_{n\rightarrow\infty}\frac{f(n)}{g(n)} = \infty
\end{equation*}

$\omega$-nodation indicates the function is bounded with a not asymptotically tight lower bound.
