
\chapter{Convolutional Networks}

\keyword{Convolutional networks}, also known as \keyword{convolutional neural networks} or CNNs, are specialized kind of neural network for processing data that has a known \keyword{grid-like} topology.
Convolution is a specialized kind of linear operation.
Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers.


\section{The Convolution Operation}

\begin{equation}
  \label{eq:convolution}
  s(t) = \int x(a)w(t-a)da.
\end{equation}
This operation is called \keyword{convolution}.
The convolution operation is typically denoted with an asterisk:
\begin{equation}
  s(t) = (x*w)(t).
\end{equation}

In convolutional network terminology, the first argument (in this example, the function $x$) to the convolution is often referred to as the \keyword{input}, and the second argument (int this example, the function $w$) as the \keyword{kernel}.
The output is sometimes referred to as the \keyword{feature map}.

If we assume that $x$ and $w$ are defined only on integer $t$, we can define the discrete convolution:
\begin{equation}
  \label{eq:discrete-convolution}
  s(t) = (x*w)(t) = \sum_{a=-\infty}^{\infty} x(a)w(t-a).
\end{equation}


We often use convolutions over more than one axis at a time.
For example, if we use a two-dimensinal image $I$ as our input, we probably also want to use a two-dimensional kernel $K$:
\begin{equation}
  S(i,j) = (I*K)(i,j) = \sum_m\sum_n I(m,n)K(i-m,j-n).
\end{equation}


Convolution is commulative, meaning we can equivalently write
\begin{equation}
  S(i,j) = (K*I)(i,j) = \sum_m\sum_n I(i-m,j-n)K(m,n).
\end{equation}

Usually the latter formula is more stratghtforward to implement in a machine learning library, because there is less variation in the range of valid values of $m$ and $n$.


The commulative property of convolution arises becuase we have fipped the kernel relative to the input, in the sense that as $m$ increase, the index into the input increase, but the index into the kernel decrease.
The only reason to flip the kernel is to obtain the commulative property.
While the commulative property is useful for writting proofs, it is not usually an important property of a neural network implementation.
Instead, many neural network libraries implement a related function called the \keyword{cross-correlation}, which is the same as convolution but without flippling the kernel:
\begin{equation}
  S(i,j) = (I*K)(i,j) = \sum_m\sum_nI(i+m,j+n)K(m,n).
\end{equation}
Many machine learning libraries implement cross-correlation but call it convolution.


\section{Motivation}

Convolution leverages three important ideas:
\begin{itemize}
\item sparse interaction.
\item parameter sharing.
\item equivariant representations.
\end{itemize}

\subsection{Sparse interaction}

Tradition neural network layers use matrix multiplication by a matrix of parameters with a separate parameter describing the interaction between each input unit and each output unit.
This means that every output unit interacts with every input unit.
Convolutional networks typically have sparse interactions.
This is accomplished by making the kernel smaller than the input.


\subsection{Parameter sharing}

Parameter sharing refers to use the same parameter for more than one function in a model.
In a traditional nerual net, each element of the weight matrix is used exactly once when computing the output of a layer.
It is multiplied by one element of the input and then never revisited.
In a convolutional neural net. each member of the kernel is used at every postion of the input.
The parameter sharing used by the convolution operation means that rather than learning a separate set of parameters for every location, we learn only one set.

\subsection{Equivariant representations}

In the case of convolution, the particular form of a parameter sharing causes the layer to have a property called \keyword{equivariance} to translation.
To say a function is equivariant means that if the input changes, the output changes in the same way.
Specifically, a function $f(x)$ is equivariant to a function $g$ if $f(g(x)) = g(f(x))$.
In the case of convolution, if we let $g$ be any function that translate the input, that is, shifts it, then the convolution function is equivalent to $g$.

For images, convolution creates a 2-D map of where certain features appear in the input.
If we move the object in the input, its representations will move the same amount in the output.
This is useful for when we know that some function of a small number of neighboring pixels is useful when applied to multiple input location.
For example, when processing images, it is useful to detect edges in the first layer of a convolutional network,
The same edges appear more or less everywhere in the image, so it is practical to share parameters across the intire image.

Convolution is not naturally equivalent to some other transformation, such as changes in the scale or ratation of a image.




\section{ Pooling}

A typical layer of a convolutional network consists of three stages:
\begin{enumerate}
\item convolution stage: affine transform
\item detector stage: nonlinearty
\item pooling stage
\end{enumerate}


In the first stage, the layer performs several convolutions in parallel to produce a set of linear activations.
In the second stage, each linear activations is run through a nonlinear activation function, such as the rectified linear activation function.
In the third stage, we use a pooling function to modify the output of the layer further.


A pooling function replaces the output of the net at a certain location with a summary statistic of the nearby outputs.
For example, the max pooling oeration reports the maximum otuput within a rectangular neighborhood.
Other popular pooling functions include the average of a rectangular neighborhood,
the $L^2$ norm of a rectangular neighborhood, or a weighted average based on the distance from the central pixel.


In all cases, pooling helps to make the representation approximately invariant to small translations of the input.
Invariant to translation means that if we translate the input by a small amount, the values of most of the pooled outputs do not change.
Invariance to local translation can be a useful property if we care about whether some feature is present than exactly where it is.
For example, when determining whether an image contains a face, we need not know the location of the eyes with pixel-perfect accuracy,
we just need to know that there is an eye on the left side of the face and an eye on the right side of the face.
In ohter example, it is more important to perserve the location of a feature.
For example, if we want to find a corner defined by two edges meeting at a specified orientation,
we need to perserve the location of the edge well enough to test whether they meet.


The use of pooling can be viewed as adding a infinitly strong prior that the function the layer learns must be invariant to small translations.
When this assumption is correct, it can greatly improve the statistical efficiency of the network.


\section{Convolution and Pooling as an Infinitely Strong Prior}

A probability distribution over the parameters of a model encodes our beliefs about what models are reasonable, before we see any data.

Prior can be considered weak or strong depending on how concertrated the probability density in the prior is.
A weak prior is a prior distribution with high entropy.
A strong prior is a prior distribution with low entropy.
An infinitely strong prior places zero probability on some parameters and says that these parameter values are completely forbidden,
regardless of how much support the data give to these values.


We can imagine a convolutional net as being similar to a fully connected net, but with an infinitely strong prior over its weights.
This infinitely strong prior says that the weights for one hidden unit must be identical to the weights of its neighbor, but shifted in space.
The prior also says that the weights must be zero, except for in the small, spatially contiguous receptive field assigned to that hidden unit.
Overall, we can think of the use of convolution as introducing an infinitely strong prior probability distribution over the parameters of a layer.
This prior says that the function the layer should learn contains only local interactions and is equivariant to translation.
Likewise, the use of pooling is an infinitely strong prior that each unit should be invariant to small translations.




\section{Structured Outputs}

Convolutional networks can be used to output a high-dimensional, structured object, rather than just predicting a class label for a classification task or a real value for a regression task.
Typically this object is just a tensor, emitted by a standard convolutional layer.
For example, the model might emit a tensor $S$, where $S_{i,j,k}$ is the probability that pixel $(j,k)$ of the input to the network belongs to class i.
This allows the model to label every pixel in an image and draw precise masks that follow the outlines of individual objects.
This the basis of the segmentation model.



In the kinds of architectures typically used for classification of a single object in an image, the greatest reduction in the spatial dimensions of the network comes from using pooling layers with large stride.
To produce an output map of similar size as the input, one can:
\begin{itemize}
\item avoid pooling altogether
\item emit a lower-resolution grid of labels
\item use a pooling operator with unit stride
\end{itemize}


